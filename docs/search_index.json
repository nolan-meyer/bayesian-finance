[["index.html", "Bayesian Finance: Modeling Earnings for S&amp;P 500 Companies Chapter 1 Welcome 1.1 Introduction", " Bayesian Finance: Modeling Earnings for S&amp;P 500 Companies Nicholas Di, Nolan Meyer, Duc Ngo Macalester College, Fall 2021, STAT 454 Capstone Project Chapter 1 Welcome 1.1 Introduction Welcome to our STAT 454: Bayesian Statistics capstone project. All of us group members have an interest in and connections to the financial world, whether that be through our majors or internships, which led us toward this topic. Financial information, like stock market prices, are known to be notoriously hard to predict. We wanted to take a Bayesian approach to try and tackle a similar situation: predicting the future earnings of S&amp;P 500 companies. In this project we seek to model future earnings using other financial information about a company, like previous earnings and sales. We explore a few Bayesian hierarchical models, as well as a bayesforcast model to try and identify one that can provide insight and better predictions for future company’s earnings. "],["data.html", "Chapter 2 Data 2.1 Data Source 2.2 Variables 2.3 Data Cleaning 2.4 Initial Explorations", " Chapter 2 Data 2.1 Data Source Our data includes financial information on companies in the S&amp;P 500 stock index from 1999-2021. This information was scraped from Yahoo Finance in November of 2021, and collected in a csv format for data analysis. To collect the data, first, we got the list of the current 500 S&amp;P companies from Slickchart (https://www.slickcharts.com/sp500). After having the list of the companies, we then moved on to the financials information of the listed companies on yahoo finance (https://finance.yahoo.com/) to get the metrics such as sales, earnings, cogs, stock price, and market sector. We then scraped the data using BeautifulSoup library in Python and turned that into a CSV file. After getting the data, our main goal is to analyze and model this data to better improve projections for a company’s future metrics, like earnings. 2.2 Variables The main variables that are used in this project are defined below: Variable Meaning YEAR The financial year of the company COMPANY The company’s stock abbreviation symbol MARKET.CAP The total market capitalization of the company (Volume * Price) EARNINGS The earnings in dollars for the previous year for the given company SALES How much the company sold in dollars last year CASH How much cash the company has in dollars at the end of the previous year Name The full name of the company Sector The name of the sector that the company is a part of Earnings_next_year The amount of money in dollars that the company earns in the following year EARNINGS_1_YEAR_AGO The amount of money in dollars that the company earned in the previous year 2.3 Data Cleaning 2.3.1 Data Pre-Processing First, as the earnings, cash and other variables are really large, we have decided to divide them by 1 billion. The reason for that is to make it easier to interpret and to understand the model. #Scaling Variables of Interest data &lt;- data %&gt;% mutate(EARNINGS_Scaled = EARNINGS/1000000000, CASH_Scaled = CASH/1000000000, MARKET.CAP_Scaled = MARKET.CAP/1000000000, Earnings_next_year_Scaled = Earnings_next_year/1000000000, SALES_Scaled = SALES/1000000000) Next, we added lagged variables for earnings and sales as we believe information from previous years will help predict future metrics. #Adding Lagged Variables data &lt;- data %&gt;% group_by(COMPANY) %&gt;% mutate(EARNINGS_1_YEAR_AGO = lead(EARNINGS_Scaled, n = 1), EARNINGS_2_YEAR_AGO = lead(EARNINGS_Scaled, n = 2), EARNINGS_3_YEAR_AGO = lead(EARNINGS_Scaled, n = 3), EARNINGS_4_YEAR_AGO = lead(EARNINGS_Scaled, n = 4)) %&gt;% mutate(SALES_1_YEAR_AGO = lead(SALES_Scaled, n = 1), SALES_2_YEAR_AGO = lead(SALES_Scaled, n = 2), SALES_3_YEAR_AGO = lead(SALES_Scaled, n = 3), SALES_4_YEAR_AGO = lead(SALES_Scaled, n = 4)) 2.3.2 Graphing Outliers We will do some more basic exploration regarding our data set, particularly our dependent variable of interest: Earnings the next year. Earnings_next_year_Scaled is the company’s earnings the next year. So Earnings_next_year_Scaled value of 1 for Company X in 2015 can be interpreted as 1 billion dollars in earnings in 2016. We explore Earnings next year below: #Graphing Outliers data %&gt;% ggplot(aes(x = Earnings_next_year_Scaled)) + geom_boxplot() data %&gt;% ggplot(aes(x = EARNINGS_Scaled, y= Earnings_next_year_Scaled))+ geom_point() There is a high number of outliers in our data, this is not ideal as we do not want to fit a model that includes outliers as predicting and modeling posterior distributions would be hard in the grand scheme. We removed outliers by classifying them as one if they are outside the range of +/- (0.9 * IQR) (Interquartile Range). Q &lt;- quantile(data$Earnings_next_year_Scaled, probs=c(.25, .75), na.rm = TRUE) iqr &lt;- IQR(data$Earnings_next_year_Scaled, na.rm = TRUE) up &lt;- Q[2]+.90*iqr # Upper Range low&lt;- Q[1]-.90*iqr # Lower Range eliminated&lt;- subset(data, data$Earnings_next_year_Scaled &gt; (low) &amp; data$Earnings_next_year_Scaled &lt; (up)) data_elimatedO &lt;- eliminated We graph the data points without outliers below: data_elimatedO %&gt;% ggplot(aes(x = Earnings_next_year_Scaled)) + geom_boxplot() data_elimatedO %&gt;% ggplot(aes(x = EARNINGS_Scaled, y= Earnings_next_year_Scaled))+ geom_point() This is much better as the data points are closer in proximity. 2.4 Initial Explorations After collecting the data, we then looked for the distribution of the companies within the S&amp;P 500. The first plot we are going to create is the number of companies within the period: As we see above, we have data for about 70% of the companies in the S&amp;P 500 for the first year of our data, and by 2021 we have about every single company within the index. It is important for us to have data on as many companies as possible over this time period so that we can better capture trends and make more accurate models based on the data. Next, we investigated how market cap, specifically the sum of all the companies’ market caps, varied from year to year. By grouping by year, we were able to easily combine each companies market cap together to create the plot above. This plot highlights trends in the overall market, we see a general increase over time in market cap, with sharp decreases around 2008 and 2020. Those two years align with the housing market crash and COVID respectively, both which led to decreases in the stock market. By identifying trends in the overall market, we may have a better idea about how individual companies may perform. Our main objective with this project is to be able to accurately predict future earnings using metrics like sales, previous earnings, and other variables like the sector of the company. We found that overall, among the top 50 companies (based on market cap), there were positive relationships between earning and sales. This relationship varies based on the market sector, with IT having the most positive relationship, and Consumer Staples having the least positive relationship. This indicates to us that both sector and sales may be important predictors of earnings that we should explore using in our future models. For most sectors, it appears that the farther back we go, the flatter the relationship between Earnings and past earnings is. If we plot earnings next year with earnings four years ago, we will see that almost all sectors have different slopes. "],["hierarchical-modeling.html", "Chapter 3 Hierarchical Modeling 3.1 Set Up 3.2 Model 1: Hierarchical w/ Different Intercepts 3.3 Model 2: Hierarchical w/ Different Slopes and Intercepts 3.4 Model Evaluations", " Chapter 3 Hierarchical Modeling 3.1 Set Up Creating a testing set Since we have time-series data, we created a testing set by sub-setting each company’s 2nd to latest year. We left off the most recent year a company reports their earnings and created a testing set with this data, the training set includes all the other previous years. We will predict earnings next year with the testing set and compare it with the actual values to help determine model accuracy. testing &lt;- data_elimatedO %&gt;% group_by(COMPANY) %&gt;% filter(row_number()==1) training &lt;- anti_join(data_elimatedO, testing) Tuning Priors We do not have a strong understanding of variability between and within companies, thus we are using weakly informative priors. This means we are using the default values in the stan_glm package. Defining Notation In our model notation below, \\(Y_{ij}\\) is earnings for the next year after the \\(i\\)th year for company \\(j\\) and \\(X_{ij}\\) is the earnings in the \\(i\\)th year for company \\(j\\). Both variables are independent. For example, Y = Earnings of Apple in 2017 in billions, and X = Earnings of Apple in 2016 in billions. 3.2 Model 1: Hierarchical w/ Different Intercepts 3.2.1 Model Structure Now, we will move on to utilizing the structure of our data, where we have consecutive observations for each company for several years. We start with a model with varying intercepts. Note: we fit each model using training data for evaluation purposes. Diff_inter_train &lt;- readRDS(&quot;Diff_inter_train_nick.rds&quot;) model_diff_inter_train_data &lt;- training %&gt;% select(c(&quot;Earnings_next_year_Scaled&quot;,&quot;EARNINGS_Scaled&quot;,&quot;EARNINGS_1_YEAR_AGO&quot;,&quot;COMPANY&quot;,&quot;Sector&quot;)) %&gt;% na.omit() model_diff_inter_train &lt;- stan_glmer( Earnings_next_year_Scaled ~ EARNINGS_Scaled + EARNINGS_1_YEAR_AGO + Sector + (1 | COMPANY) , data = model_diff_inter_train_data, family = gaussian, chains = 4, iter = 5000*2, seed = 84735, prior_PD = FALSE, refresh = 0) write_rds(model_diff_inter_train, &quot;Diff_Inter_train_nick.rds&quot;) prior_summary(Diff_inter_train) Below is the notation for the differing intercepts model: \\[\\begin{split} \\text{Relationship within company:} &amp; \\\\ Y_{ij} | \\beta_{0j}, \\beta_1, \\sigma_y &amp; \\sim N(\\mu_{ij}, \\sigma_y^2) \\;\\; \\text{ where } \\mu_{ij} = \\beta_{0j} + \\beta_1 X_{ij} + \\beta_2 X_{ij} + \\beta_3 X_{ij}...\\\\ &amp; \\\\ \\text{Variability between companies:} &amp; \\\\ \\beta_{0j} &amp; \\stackrel{ind}{\\sim} N(\\beta_0, \\sigma_0^2) \\\\ &amp; \\\\ \\text{Prior information on Globals with Adjusted Prior} &amp; \\\\ \\beta_{0c} &amp; \\sim N(0.59, 1.5^2) \\\\ \\beta_1 &amp; \\sim N(0, 1.74^2) \\\\ \\beta_2 &amp; \\sim N(0, 1.60^2) \\\\ \\beta_3 &amp; \\sim N(0, 4.60^2) \\\\ .\\\\ .\\\\ .\\\\ \\sigma_y &amp; \\sim \\text{Exp}(1.6) \\\\ \\sigma_0 &amp; \\sim \\text{Exp}(1) \\\\ \\end{split}\\] 3.3 Model 2: Hierarchical w/ Different Slopes and Intercepts 3.3.1 Model Structure In addition to having a hierarchical regression with different intercepts, we decided to add a model with different intercepts and slopes. Rational behind different slopes: Below we graph 4 random companies, we can see that earnings in the current year impacts earnings next year differently among different companies. eliminated %&gt;% filter(COMPANY %in% c(&quot;AAL&quot;,&quot;CVS&quot;,&quot;DAL&quot;,&quot;WAB&quot;)) %&gt;% ggplot(., aes(x = EARNINGS_Scaled, y = Earnings_next_year_Scaled)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + facet_grid(~ COMPANY) To get a better idea of the varying slopes, I graph 50 random companies together. vector &lt;- eliminated$COMPANY vector &lt;- sample_n(as.data.frame(vector), 50) vector &lt;- as.list(vector) eliminated %&gt;% filter(COMPANY %in% vector$vector) %&gt;% ggplot(aes(x=EARNINGS_Scaled, y= Earnings_next_year_Scaled, group = COMPANY))+ geom_smooth(method = &quot;lm&quot;, se= FALSE, size = 0.5) We believe it makes sense to replace the global earnings coefficient with a company specific earnings coefficient. Diff_inter_slope_train &lt;- readRDS(&quot;Diff_inter_slope_train_nick.rds&quot;) model_diff_inter_slope_train_data &lt;- training %&gt;% select(c(&quot;Earnings_next_year_Scaled&quot;,&quot;Sector&quot;,&quot;COMPANY&quot;,&quot;EARNINGS_Scaled&quot;,&quot;EARNINGS_1_YEAR_AGO&quot;)) %&gt;% na.omit() diff_slope_inter_model_train &lt;- stan_glmer( Earnings_next_year_Scaled ~ EARNINGS_Scaled + EARNINGS_1_YEAR_AGO + (EARNINGS_Scaled | COMPANY) + Sector, data = model_diff_inter_slope_train_data, family = gaussian, chains = 4, iter = 5000*2, seed = 84735, prior_PD = FALSE) write_rds(diff_slope_inter_model_train, &quot;Diff_inter_slope_train_nick.rds&quot;) prior_summary(Diff_inter_slope_train) Different intercepts &amp; slopes model notation: \\[\\begin{split} Y_{ij} | \\beta_{0j}, \\beta_{1j}, \\sigma_y &amp; \\sim N(\\mu_{ij}, \\sigma_y^2) \\;\\; \\text{ where } \\; \\mu_{ij} = \\beta_{0j} + \\beta_{1j} X_{ij} + \\beta_{2} X_{ij} + \\beta_{3} X_{ij}...\\\\ &amp; \\\\ \\beta_{0j} &amp; \\sim N(\\beta_0, \\sigma_0^2) \\\\ \\beta_{1j} &amp; \\sim N(\\beta_1, \\sigma_1^2) \\\\ &amp; \\\\ \\beta_{0c} &amp; \\sim N(0.59, 1.5^2) \\\\ \\beta_1 &amp; \\sim N(0, 1.74^2) \\\\ .\\\\ .\\\\ .\\\\ \\sigma_y &amp; \\sim \\text{Exp}(1.6) \\\\ \\sigma_0, \\sigma_1, ... &amp; \\sim \\text{(something a bit complicated)}. \\\\ \\end{split}\\] 3.4 Model Evaluations 3.4.1 Is this the right model? Using our models we simulate replicated data and then compare these to the observed data to look for discrepancies between the two in the plots below. pp_check(Diff_inter_train) pp_check(Diff_inter_slope_train) Several company earnings’ on the right seem to be causing model fitness difficulties. Both models run into this problem. There seems to be little difference between the two models in terms of fitting the structure of earnings next year. We will now dive into predicting the accuracy of the models. 3.4.2 How Accurate are the models? Model 1 - Specific Examples with companies: Below, we compare our predictions for American Airlines. We plot 750 random values predicted from our predictions (out of 20,000). As we can see below our predictions range cover the actual value of earnings for 2020 fiscal year for both models. set.seed(84732) predict_model(&quot;AAL&quot;, Diff_inter_train) # mcmc_areas(predict_next_year, prob = 0.8) + # ggplot2::scale_y_discrete(labels = c(`test_comp`)) + geom_vline(xintercept = actual, linetype = &quot;dashed&quot;, colour = &quot;red&quot;) #Need Help Because X scale is different Model 2 - Specific Examples with companies: set.seed(84732) predict_model(&quot;AAL&quot;, Diff_inter_slope_train) # mcmc_areas(predict_next_year, prob = 0.8) + # ggplot2::scale_y_discrete(labels = c(`test_comp`)) + geom_vline(xintercept = actual, linetype = &quot;dashed&quot;, colour = &quot;red&quot;) The predictions for the second model seem to be slightly better as more posterior predictive points are near the actual value for “2020”. Evaluating Metrics For the prediction metrics, we calculated 20,000 values for each individual company using the hierarchical models. We then take the median value of the 20,000 for each company and calculate the mean distance from the median to the actual earnings observed for that year. We also computed the 95% and 50% prediction intervals by calculating the percentage of 20,000 predicted values are within the 2.5th and 97.5th percentile and 25 to 75th percentile respectively. Different Intercepts Model: Diff_inter_metrics ## MAE Within95 Within50 ## 1 0.3620034 0.7543103 0.4181034 Different Intercepts &amp; Slopes Model: Diff_inter_slope_metrics ## MAE Within95 Within50 ## 1 0.2781092 0.7931034 0.4935345 We can see that our model with varying intercepts and slopes (model 2) preforms slightly better. Where our average median posterior prediction is off by 0.28 billion as opposed to 0.362 billion when we only have differing intercepts. Furthermore, our 95 and 50 interval values are both better in the model with different intercept and slope. Shrinkage Since we modeled based off different companies having different intercepts, it is worthwhile to checkout how the company baselines shrunk compared to each other and between the two different models. We randomly sampled 70 companies, since if we plot all companies we will have more than 400 companies on the X-axis. We can visually see how the intercepts become less varied as we are looking at the hierarchical model with different intercept and slopes. Model 1 Shrinkage: Model 2 Shrinkage: 3.4.3 Interpreting Coefficents: NOT COMPLETED tidy(Diff_inter_train, effects = &quot;fixed&quot;, conf.int = TRUE, conf.level = .80) ## # A tibble: 13 × 5 ## term estimate std.error conf.low conf.high ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 0.320 0.0744 0.225 0.417 ## 2 EARNINGS_Scaled 0.222 0.00686 0.213 0.230 ## 3 EARNINGS_1_YEAR_AGO 0.0961 0.00613 0.0883 0.104 ## 4 SectorConsumer Discretionary 0.115 0.0856 0.00449 0.225 ## 5 SectorConsumer Staples 0.341 0.0998 0.212 0.469 ## 6 SectorEnergy 0.337 0.112 0.195 0.480 ## 7 SectorFinancials 0.288 0.0862 0.178 0.400 ## 8 SectorHealth Care 0.0608 0.0865 -0.0500 0.173 ## 9 SectorIndustrials 0.132 0.0843 0.0248 0.242 ## 10 SectorInformation Technology 0.0635 0.0844 -0.0449 0.174 ## 11 SectorMaterials 0.0971 0.0989 -0.0316 0.224 ## 12 SectorReal Estate -0.0825 0.0959 -0.207 0.0401 ## 13 SectorUtilities 0.234 0.0962 0.110 0.357 tidy(Diff_inter_slope_train, effects = &quot;fixed&quot;, conf.int = TRUE, conf.level = .80) ## # A tibble: 13 × 5 ## term estimate std.error conf.low conf.high ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 0.308 0.0555 0.236 0.378 ## 2 EARNINGS_Scaled 0.547 0.0203 0.521 0.573 ## 3 EARNINGS_1_YEAR_AGO 0.0408 0.00697 0.0320 0.0500 ## 4 SectorConsumer Discretionary -0.0272 0.0630 -0.107 0.0536 ## 5 SectorConsumer Staples 0.0583 0.0726 -0.0339 0.152 ## 6 SectorEnergy 0.181 0.0794 0.0778 0.283 ## 7 SectorFinancials 0.119 0.0623 0.0397 0.201 ## 8 SectorHealth Care 0.000288 0.0636 -0.0801 0.0823 ## 9 SectorIndustrials 0.0192 0.0619 -0.0592 0.0996 ## 10 SectorInformation Technology -0.0370 0.0613 -0.116 0.0419 ## 11 SectorMaterials -0.0381 0.0704 -0.128 0.0521 ## 12 SectorReal Estate -0.128 0.0697 -0.217 -0.0371 ## 13 SectorUtilities 0.0597 0.0685 -0.0272 0.148 Global Standard Deviation Parameters: NOT COMPLETED ADFASGARGERGEARG tidy(model_2, effects = &quot;ran_pars&quot;) (0.4512^2)/((0.4512^2) + (0.5519447^2)) tidy(model_3, effects = &quot;ran_pars&quot;) (0.4294^2)/((0.4294^2) + (0.5450631)^2) In model 2, about 40.06% of the variance can be explained between companies. In model 3, about 38.29% of the variance can be explained between companies. This means that when including sectors and lagged variables, more variable shrinkage occurred. Unfortunately this shrinkage is not as evident in the plotted graphs above. Model 3’s baseline intercepts differ among each other slightly less than model 2’s. "],["bayes-forecast.html", "Chapter 4 Bayes Forecast 4.1 Model Parameters 4.2 Modeling", " Chapter 4 Bayes Forecast library(bayesforecast) Bayesian Forecast SARIMA: In our last model specification, we decided to use the bayesforecast package. In particular we are fitting a SARIMA model in Stan. SARIMA stands for seasonal auto-regressive integrated moving average. This is an extension of ARIMA and is therefore more robust as it is able to support seasonal data. ARIMA is a method that combines both auto-regressive methods and moving averages- it is widely used on time series data in attempts to predict future values. There are four components that explain time series data, trend, seasonality, irregularity, and cyclic components. Reason to use Bayesian forecast: The reason that we are using Bayesian forecast is that we could see the prediction of the earnings for not only the next year but for 10 years in the future. Bayesian forecast was created based on seasonal order, trend or seasonality, which could predict the model fairly accurately. After running the model, we then move on to predict the earnings of the future year for the companies. The companies we are going to predict will be Amazon (symbol: “AMZN”), American Airlines (symbol: “AAL”), and Koka Kola (symbol: “KO”). 4.1 Model Parameters Here are the parameters for an ARIMA model: P - Order of the AR term. This is the number of Y to be used as predictors. For example, if we are predicting 2021 earnings, how many previous years earnings are we going to use? Q - Order of the MA term. This is the number of lagged forecast errors. How many past forecast errors will we be using? D - The minimum differencing period. A stationary time series implies one that has properties that do not depend on the time at which the series is observed. As mentioned above, SARIMA is able to support seasonal data. Below are the parameters for a SARIMA model that ARIMA does not have. P - Seasonal autoregressive order. A P=1 would make use of the first seasonally offset observation in the model, e.g. t-(m1) or t-12. A P=2, would use the last two seasonally offset observations t-(m1), t-(m2). D - Seasonal difference order. A D of 1 would calculate a first order seasonal difference and a Q=1 would use a first order error in the model (e.g. moving average). Q - Seasonal moving average order. M - The number of time steps for a single seasonal period. M is a very important parameter as it influences the P, D, and Q parameters. For example, an m of 5 for yearly data suggests a 5-year seasonal cycle (in the context of business cycles. 4.2 Modeling After creating the model, we will then move on to predict the earnings of the future year for the companies. The companies we are going to predict will be Amazon (symbol: “AMZN”), American Airlines (symbol: “AAL”), and Coca Cola (symbol: “KO”). Amazon Prediction AMZN &lt;- data %&gt;% filter(COMPANY == &#39;AMZN&#39;) %&gt;% dplyr::select(EARNINGS_Scaled) %&gt;% arrange(EARNINGS_Scaled) vector &lt;- AMZN$EARNINGS_Scaled myts &lt;- ts(vector, start=c(1999), end=c(2021), frequency=1) sf_AMZN = stan_sarima(ts = myts,order = c(1,1,1),seasonal = c(1,1,1), prior_mu0 = student(mu = 0,sd = 1,df = 7)) autoplot(forecast(object = sf_AMZN, h = 12)) First, for Amazon, we see that the earnings are predicted to increase from 2021 to 2030, moving from 21 billion to around 38 billion in 2030. However, we can see that the prediction states that the earnings growth tends to slow down over time over the period. American Airlines Prediction AAL &lt;- data %&gt;% filter(COMPANY == &#39;AAL&#39;) %&gt;% dplyr::select(EARNINGS_Scaled) %&gt;% arrange(EARNINGS_Scaled) vector &lt;- AAL$EARNINGS_Scaled myts &lt;- ts(vector, start=c(1999), end=c(2021), frequency=1) sf_AAL = stan_sarima(ts = myts,order = c(1,1,1),seasonal = c(1,1,1), prior_mu0 = student(mu = 0,sd = 1,df = 7)) autoplot(forecast(object = sf_AAL, h = 12)) For American Airlines, we see a different story. As we can see from the graph, as airlines industry is an extremely cyclical field, the earnings fluctuate a lot. We could see that they fluctuate a lot during the previous years. With that in mind, the model predicts that American Airlines will not improve the much during the following years from 2021 to 2030 as predicting a cyclical company’s earnings could be a really difficult story. It did not show the similar pattern as Amazon. Coca Cola Prediction KO &lt;- data %&gt;% filter(COMPANY == &#39;KO&#39;) %&gt;% dplyr::select(EARNINGS_Scaled) %&gt;% arrange(EARNINGS_Scaled) vector_KO &lt;- KO$EARNINGS_Scaled myts_KO &lt;- ts(vector_KO, start=c(1999), end=c(2021), frequency=1) sf_KO = stan_sarima(ts = myts_KO,order = c(1,1,1),seasonal = c(1,1,1), prior_mu0 = student(mu = 0,sd = 1,df = 7)) autoplot(forecast(object = sf_KO,h = 12)) For Coca Cola, it follows the same case as American Airlines. Even though the earnings increase over time, the model predicts that Ford’s earning will stay relatively the same throughout the year from 2021 to 2030. It should be fairly clear as Coca Cola is a company that has developed for a really long period of time and can be considered to be a dividend company. With that in mind, it is fairly reasonable to see the earnings of the company to stay relatively the same after a period. "],["finalmodel.html", "Chapter 5 Final Model", " Chapter 5 Final Model "],["reflection.html", "Chapter 6 Reflection 6.1 Next Steps 6.2 Acknowledgements 6.3 Citations 6.4 Code Appendix", " Chapter 6 Reflection 6.1 Next Steps 6.2 Acknowledgements 6.3 Citations “Modeltime Integration.” Bayesmodels, https://albertoalmuinha.github.io/bayesmodels/articles/modeltime-integration.html. S&amp;P 500 Companies - S&amp;P 500 Index Components by Market Cap, https://www.slickcharts.com/sp500 “Yahoo Finance - Stock Market Live, Quotes, Business &amp; Finance News.” Yahoo! Finance, Yahoo!, https://finance.yahoo.com/. 6.4 Code Appendix To reproduce these results, the complete files and data are provided below: xfun::embed_files(c(&#39;data.Rmd&#39;, &#39;modeling.Rmd&#39;, &#39;finalmodel.Rmd&#39;, &#39;FINALDATASET.csv&#39;, &#39;RandomCompnay.csv&#39;, &#39;Diff_Inter_train_nick.rds&#39;, &#39;Diff_inter_slope_train_nick.rds&#39;)) "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
