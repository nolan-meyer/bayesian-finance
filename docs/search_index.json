[["index.html", "Bayesian Finance: Modeling Earnings for S&amp;P 500 Companies Chapter 1 Welcome 1.1 Introduction", " Bayesian Finance: Modeling Earnings for S&amp;P 500 Companies Nicholas Di, Nolan Meyer, Duc Ngo Macalester College, Fall 2021, STAT 454 Capstone Project Chapter 1 Welcome 1.1 Introduction Welcome to our STAT 454: Bayesian Statistics capstone project. All of us group members have an interest in and connections to the financial world, whether that be through our majors or internships, which led us toward this topic. Financial information, like stock market prices, are known to be notoriously hard to predict. We wanted to take a Bayesian approach to try and tackle a similar situation: predicting the future earnings of S&amp;P 500 companies. In this project we seek to model future earnings using other financial information about a company, like previous earnings and sales. We explore a few Bayesian hierarchical models, as well as a SARIMA model using the bayesforcast package to try and identify one that can provide insight and better predictions for future company’s earnings. "],["data.html", "Chapter 2 Data 2.1 Data Source 2.2 Variables 2.3 Data Cleaning 2.4 Initial Explorations", " Chapter 2 Data 2.1 Data Source Our data includes financial information on companies in the S&amp;P 500 stock index from 1999-2021. This information was scraped from Yahoo Finance in November of 2021, and collected in a csv format for data analysis. To collect the data, first, we got the list of the current 500 S&amp;P companies from Slickchart (https://www.slickcharts.com/sp500). After having the list of the companies, we then moved on to the financial information of the listed companies on yahoo finance (https://finance.yahoo.com/) to get the metrics such as sales, earnings, cogs, stock price, and market sector. We then scraped the data using the BeautifulSoup library in Python and turned that into a CSV file. Our main goal is to analyze and model this data to better improve projections for a company’s future metrics, like earnings. 2.2 Variables The main variables that are used in this project are defined below: Variable Meaning YEAR The financial year of the company COMPANY The company’s stock abbreviation symbol MARKET.CAP The total market capitalization of the company (Volume * Price) EARNINGS The earnings in dollars for the previous year for the given company SALES How much the company sold in dollars last year CASH How much cash the company has in dollars at the end of the previous year Name The full name of the company Sector The name of the sector that the company is a part of Earnings_next_year The amount of money in dollars that the company earns in the following year EARNINGS_1_YEAR_AGO The amount of money in dollars that the company earned in the previous year 2.3 Data Cleaning 2.3.1 Data Pre-Processing First, as earnings, cash, and other variables are really large, we have decided to divide them by 1 billion. The reason for this is to make it easier to interpret and to understand the model. #Scaling Variables of Interest data &lt;- data %&gt;% mutate(EARNINGS_Scaled = EARNINGS/1000000000, CASH_Scaled = CASH/1000000000, MARKET.CAP_Scaled = MARKET.CAP/1000000000, Earnings_next_year_Scaled = Earnings_next_year/1000000000, SALES_Scaled = SALES/1000000000) Next, we added lagged variables for earnings and sales as we believe information from previous years will help predict future metrics. #Adding Lagged Variables data &lt;- data %&gt;% group_by(COMPANY) %&gt;% mutate(EARNINGS_1_YEAR_AGO = lead(EARNINGS_Scaled, n = 1), EARNINGS_2_YEAR_AGO = lead(EARNINGS_Scaled, n = 2), EARNINGS_3_YEAR_AGO = lead(EARNINGS_Scaled, n = 3), EARNINGS_4_YEAR_AGO = lead(EARNINGS_Scaled, n = 4)) %&gt;% mutate(SALES_1_YEAR_AGO = lead(SALES_Scaled, n = 1), SALES_2_YEAR_AGO = lead(SALES_Scaled, n = 2), SALES_3_YEAR_AGO = lead(SALES_Scaled, n = 3), SALES_4_YEAR_AGO = lead(SALES_Scaled, n = 4)) 2.3.2 Graphing Outliers We will do some more basic exploration regarding our data set, particularly our dependent variable of interest: Earnings next year. Earnings_next_year_Scaled is the company’s earnings the next year, so a value of 1 for Company X in 2015 can be interpreted as Company X had 1 billion dollars in earnings in 2016. We explore Earnings next year below: #Graphing Outliers data %&gt;% ggplot(aes(x = Earnings_next_year_Scaled)) + geom_boxplot() data %&gt;% ggplot(aes(x = EARNINGS_Scaled, y= Earnings_next_year_Scaled))+ geom_point() There are a high number of outliers in our data, this is not ideal as we do not want to fit a model that includes many outliers, as predicting and modeling these posterior distributions would be quite difficult. We decided to remove outliers by classifying them as one if they are outside the range of +/- (0.9 * IQR) (Interquartile Range). Q &lt;- quantile(data$Earnings_next_year_Scaled, probs=c(.25, .75), na.rm = TRUE) iqr &lt;- IQR(data$Earnings_next_year_Scaled, na.rm = TRUE) up &lt;- Q[2]+.90*iqr # Upper Range low&lt;- Q[1]-.90*iqr # Lower Range eliminated&lt;- subset(data, data$Earnings_next_year_Scaled &gt; (low) &amp; data$Earnings_next_year_Scaled &lt; (up)) data_elimatedO &lt;- eliminated We graph the remaining data points without outliers below: data_elimatedO %&gt;% ggplot(aes(x = Earnings_next_year_Scaled)) + geom_boxplot() data_elimatedO %&gt;% ggplot(aes(x = EARNINGS_Scaled, y= Earnings_next_year_Scaled))+ geom_point() This is much better as the data points are closer in proximity. 2.4 Initial Explorations After cleaning the data, we then did some initial explorations like looking at the distribution of the companies within the S&amp;P 500. The first plot we are going to create is the number of companies within the given time period: tabyl(data$YEAR) %&gt;% ggplot(aes(x= `data$YEAR`, y = n)) + geom_line() + labs(title = &quot;Number of Companies Within the Period&quot;, x = &quot;&quot;, y = &quot;&quot;) + theme_minimal() As we see above, we have data for about 70% of the companies in the S&amp;P 500 for the first year of our data, and by 2021 we have about every single company within the index. It is important for us to have data on as many companies as possible over this time period so that we can better capture trends and make more accurate models based on the data. sumUSA &lt;- data %&gt;% group_by(YEAR) %&gt;% summarise(sumMarket_cap = sum(`MARKET.CAP`), sumEarnings = sum(EARNINGS), sumSALES = sum(SALES), sumCASH = sum(CASH)) sumUSA %&gt;% ggplot(aes(x = YEAR, y = sumMarket_cap)) + geom_line() + theme_minimal() + labs(x = &quot;Year&quot;, y = &quot;Sum of Market Cap&quot;, title = &quot;Market Cap within Dataset&quot;) Next, we investigated how market cap, specifically the sum of all the companies’ market caps, varied from year to year. By grouping by year, we were able to easily combine each companies market cap together to create the plot above. This plot highlights trends in the overall market, we see a general increase over time in market cap, with sharp decreases around 2008 and 2020. Those two years align with the housing market crash and the emergence of COVID-19 respectively, both which led to decreases in the overall stock market. By identifying trends in the overall market, we may have a better idea about how individual companies may perform. temp &lt;- data %&gt;% group_by(COMPANY) %&gt;% summarize(count = n(), mean_MC = mean(MARKET.CAP)) %&gt;% filter(count == 23) %&gt;% arrange(desc(mean_MC)) %&gt;% head(50) temp &lt;- temp$COMPANY data_2 &lt;- data %&gt;% filter(COMPANY %in% temp) data_2 %&gt;% ggplot(aes(y = EARNINGS, x= SALES, color = Sector))+ geom_point(alpha = 0.20)+ geom_smooth(method = &#39;lm&#39;, formula = y ~ x)+ theme_minimal()+ ggtitle(&quot;Sales and Earnings Relationship by Sector \\n Among top 50 Companies by Market Cap&quot;) Our main objective with this project is to be able to accurately predict future earnings using metrics like sales, previous earnings, and other variables like the sector of the company. We found that overall, among the top 50 companies (based on market cap), there were positive relationships between earning and sales. This relationship varies based on the market sector, with IT having the most positive relationship, and Consumer Staples having the least positive relationship. This indicates to us that both sector and sales may be important predictors of earnings that we should explore using in our future models. p1 &lt;- data %&gt;% ggplot(aes(y = Earnings_next_year_Scaled, x= EARNINGS_Scaled, color = Sector))+ geom_point(alpha = 0.20)+ geom_smooth(method = &#39;lm&#39;, formula = y ~ x)+ theme_minimal()+ ggtitle(&quot;Same Year&quot;) p2 &lt;- data %&gt;% ggplot(aes(y = Earnings_next_year_Scaled, x= EARNINGS_1_YEAR_AGO, color = Sector))+ geom_point(alpha = 0.20)+ geom_smooth(method = &#39;lm&#39;, formula = y ~ x)+ theme_minimal()+ ggtitle(&quot;1 year ago&quot;) p3 &lt;- data %&gt;% ggplot(aes(y = Earnings_next_year_Scaled, x= EARNINGS_2_YEAR_AGO, color = Sector))+ geom_point(alpha = 0.20)+ geom_smooth(method = &#39;lm&#39;, formula = y ~ x)+ theme_minimal()+ ggtitle(&quot;2 year ago&quot;) p4 &lt;- data %&gt;% ggplot(aes(y = Earnings_next_year_Scaled, x= EARNINGS_3_YEAR_AGO, color = Sector))+ geom_point(alpha = 0.20)+ geom_smooth(method = &#39;lm&#39;, formula = y ~ x)+ theme_minimal()+ ggtitle(&quot;3 year ago&quot;) p5 &lt;- data %&gt;% ggplot(aes(y = Earnings_next_year_Scaled, x= EARNINGS_4_YEAR_AGO, color = Sector))+ geom_point(alpha = 0.20)+ geom_smooth(method = &#39;lm&#39;, formula = y ~ x)+ theme_minimal()+ ggtitle(&quot;4 year ago&quot;) ggarrange(p1, p2, p3, p4, p5, ncol = 3, nrow = 2, common.legend = TRUE, legend = &quot;bottom&quot;) For most sectors, it appears that the farther back we go, the flatter the relationship between Earnings and past earnings is. If we plot earnings next year with earnings four years ago, we will see that almost all sectors have different slopes. This indicates to us that more recent years (earnings 1 year ago, 2 years ago, etc.) will be more useful in our models than less recent years (earnings 4 years ago, 5 years ago, etc.) "],["hierarchical-modeling.html", "Chapter 3 Hierarchical Modeling 3.1 Set Up 3.2 Model 1: Hierarchical w/ Different Intercepts 3.3 Model 2: Hierarchical w/ Different Slopes and Intercepts 3.4 Model Evaluations", " Chapter 3 Hierarchical Modeling 3.1 Set Up Creating a testing set Since we have time-series data, we created a testing set by sub-setting each company’s 2nd to latest year. We left off the most recent year a company reports their earnings and created a testing set with this data, the training set includes all the other previous years. We will predict the earnings next year for each company with the testing set and compare it with the actual values to help determine model accuracy. testing &lt;- data_elimatedO %&gt;% group_by(COMPANY) %&gt;% filter(row_number()==1) training &lt;- anti_join(data_elimatedO, testing) Tuning Priors Earnings, like most things in the financial markets are quite volatile, making them harder to model. We do not have a strong understanding of the variability between and within companies, thus we are using weakly informative priors. This means that the models will rely more on the data to form the posterior distributions. We are using the default prior values found in the stan_glmer package. Defining Notation In our model notation below, \\(Y_{ij}\\) is earnings for the next year after the \\(i\\)th year for company \\(j\\) and \\(X_{ij}\\) is the earnings in the \\(i\\)th year for company \\(j\\). For example, Y = Earnings of Apple in 2017 in billions, and X = Earnings of Apple in 2016 in billions. 3.2 Model 1: Hierarchical w/ Different Intercepts 3.2.1 Model Structure Now, we will move on to utilizing the structure of our data, where we have consecutive observations for each company for several years. We start with a model with varying intercepts. Note: we fit each model using training data for evaluation purposes. Diff_inter_train &lt;- readRDS(&quot;Diff_inter_train_nick.rds&quot;) model_diff_inter_train_data &lt;- training %&gt;% select(c(&quot;Earnings_next_year_Scaled&quot;,&quot;EARNINGS_Scaled&quot;,&quot;EARNINGS_1_YEAR_AGO&quot;,&quot;COMPANY&quot;,&quot;Sector&quot;)) %&gt;% na.omit() model_diff_inter_train &lt;- stan_glmer( Earnings_next_year_Scaled ~ EARNINGS_Scaled + EARNINGS_1_YEAR_AGO + Sector + (1 | COMPANY) , data = model_diff_inter_train_data, family = gaussian, chains = 4, iter = 5000*2, seed = 84735, prior_PD = FALSE, refresh = 0) write_rds(model_diff_inter_train, &quot;Diff_Inter_train_nick.rds&quot;) prior_summary(Diff_inter_train) Below is the notation for the differing intercepts model: \\[\\begin{split} \\text{Relationship within company:} &amp; \\\\ Y_{ij} | \\beta_{0j}, \\beta_1, \\sigma_y &amp; \\sim N(\\mu_{ij}, \\sigma_y^2) \\;\\; \\text{ where } \\mu_{ij} = \\beta_{0j} + \\beta_1 X_{ij} + \\beta_2 X_{ij} + \\beta_3 X_{ij}...\\\\ &amp; \\\\ \\text{Variability between companies:} &amp; \\\\ \\beta_{0j} &amp; \\stackrel{ind}{\\sim} N(\\beta_0, \\sigma_0^2) \\\\ &amp; \\\\ \\text{Prior information on Globals with Adjusted Prior} &amp; \\\\ \\beta_{0c} &amp; \\sim N(0.59, 1.5^2) \\\\ \\beta_1 &amp; \\sim N(0, 1.74^2) \\\\ \\beta_2 &amp; \\sim N(0, 1.60^2) \\\\ \\beta_3 &amp; \\sim N(0, 4.60^2) \\\\ .\\\\ .\\\\ .\\\\ \\sigma_y &amp; \\sim \\text{Exp}(1.6) \\\\ \\sigma_0 &amp; \\sim \\text{Exp}(1) \\\\ \\end{split}\\] 3.3 Model 2: Hierarchical w/ Different Slopes and Intercepts 3.3.1 Model Structure In addition to having a hierarchical regression with different intercepts, we decided to add a model with different intercepts and slopes. Rational behind different slopes: Below we graph 4 random companies, we can see that earnings in the current year impacts earnings next year differently among different companies. eliminated %&gt;% filter(COMPANY %in% c(&quot;AAL&quot;,&quot;CVS&quot;,&quot;DAL&quot;,&quot;WAB&quot;)) %&gt;% ggplot(., aes(x = EARNINGS_Scaled, y = Earnings_next_year_Scaled)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + facet_grid(~ COMPANY) To get a better idea of the varying slopes, we graphed 50 random companies together. vector &lt;- eliminated$COMPANY vector &lt;- sample_n(as.data.frame(vector), 50) vector &lt;- as.list(vector) eliminated %&gt;% filter(COMPANY %in% vector$vector) %&gt;% ggplot(aes(x=EARNINGS_Scaled, y= Earnings_next_year_Scaled, group = COMPANY))+ geom_smooth(method = &quot;lm&quot;, se= FALSE, size = 0.5) Instead of using a global earnings slope coefficient, we decide to add a hierarchical layer so that company slope coefficients can learn from each other and yet have their own specific slope. If we have a universal slope coefficient, there will be a great deal of bias within our model. Diff_inter_slope_train &lt;- readRDS(&quot;Diff_inter_slope_train_nick.rds&quot;) model_diff_inter_slope_train_data &lt;- training %&gt;% select(c(&quot;Earnings_next_year_Scaled&quot;,&quot;Sector&quot;,&quot;COMPANY&quot;,&quot;EARNINGS_Scaled&quot;,&quot;EARNINGS_1_YEAR_AGO&quot;)) %&gt;% na.omit() diff_slope_inter_model_train &lt;- stan_glmer( Earnings_next_year_Scaled ~ EARNINGS_Scaled + EARNINGS_1_YEAR_AGO + (EARNINGS_Scaled | COMPANY) + Sector, data = model_diff_inter_slope_train_data, family = gaussian, chains = 4, iter = 5000*2, seed = 84735, prior_PD = FALSE) write_rds(diff_slope_inter_model_train, &quot;Diff_inter_slope_train_nick.rds&quot;) prior_summary(Diff_inter_slope_train) Different intercepts &amp; slopes model notation: \\[\\begin{split} Y_{ij} | \\beta_{0j}, \\beta_{1j}, \\sigma_y &amp; \\sim N(\\mu_{ij}, \\sigma_y^2) \\;\\; \\text{ where } \\; \\mu_{ij} = \\beta_{0j} + \\beta_{1j} X_{ij} + \\beta_{2} X_{ij} + \\beta_{3} X_{ij}...\\\\ &amp; \\\\ \\beta_{0j} &amp; \\sim N(\\beta_0, \\sigma_0^2) \\\\ \\beta_{1j} &amp; \\sim N(\\beta_1, \\sigma_1^2) \\\\ &amp; \\\\ \\beta_{0c} &amp; \\sim N(0.59, 1.5^2) \\\\ \\beta_1 &amp; \\sim N(0, 1.74^2) \\\\ .\\\\ .\\\\ .\\\\ \\sigma_y &amp; \\sim \\text{Exp}(1.6) \\\\ \\sigma_0, \\sigma_1, ... &amp; \\sim \\text{(something a bit complicated)}. \\\\ \\end{split}\\] 3.4 Model Evaluations 3.4.1 Is this the right model? Using our models we simulate replicated data and then compare these to the observed data to look for discrepancies between the two in the plots below. pp_check(Diff_inter_train) pp_check(Diff_inter_slope_train) Several company earnings’ on the right seem to be causing model fitness difficulties. Both models run into this problem, however the second model is able to capture more of the outliers on the right. Prior to removing the extreme outliers in our data set, the posterior predictive check for the models were worse as it left majority of the outliers uncovered. After filtering out for outliers, we evidently still have problems with both models. Next we will examine the predictive accuracy of the models. 3.4.2 How Accurate are the models? Below, we compare our predictions for American Airlines, Delta Airlines, and Amazon. We plot 750 random values predicted from our predictions (out of 20,000) for both models. We can see below our predictions for each of these companies for both hierarchical models. Model 1 - Specific Examples with companies: set.seed(84732) predict_model(&quot;AAL&quot;, Diff_inter_train) set.seed(84732) predict_model(&quot;DAL&quot;, Diff_inter_train) set.seed(84732) predict_model(&quot;AMZN&quot;, Diff_inter_train) Model 2 - Specific Examples with companies: set.seed(84732) predict_model(&quot;AAL&quot;, Diff_inter_slope_train) set.seed(84732) predict_model(&quot;DAL&quot;, Diff_inter_slope_train) set.seed(84732) predict_model(&quot;AMZN&quot;, Diff_inter_slope_train) The predictions for the second model (different intercepts and slopes) seem to be slightly better as more posterior predictive points are nearer to the actual values of earnings in “2020”. Notice how for both models, predictions for Delta Air Lines are completely off from the actual data. However, in our model with different intercepts and slopes, more predictions are closer to the actual value, thus having the model mean being closer as well. Evaluating Metrics For the prediction metrics, using the training data set, we calculated 20,000 values for each individual company for their earnings next year (the last year we have data on them) using the hierarchical models. We then take the median value of the 20,000 for each company and calculate the mean distance from the median to the actual earnings observed for that year. We also computed the 95% and 50% prediction intervals by calculating the percentage of 20,000 predicted values are within the 2.5th and 97.5th percentile and 25 to 75th percentile respectively. Different Intercepts Model: Diff_inter_metrics ## MAE Within95 Within50 Within90 ## 1 0.3620034 0.7543103 0.4181034 0.7090517 Different Intercepts &amp; Slopes Model: Diff_inter_slope_metrics ## MAE Within95 Within50 Within90 ## 1 0.2781092 0.7931034 0.4935345 0.7262931 We can see that our model with varying intercepts and slopes (model 2) preforms slightly better. Where our average median posterior prediction is off by 0.28 billion as opposed to 0.362 billion when we only have differing intercepts. Furthermore, our 95 and 50 interval values are both better in the model with different intercept and slope. Shrinkage Since we modeled based off different companies having different intercepts, it is worthwhile to checkout how the company baselines shrunk compared to each other and between the two different models. We randomly sampled 70 companies, since if we plot all companies we will have more than 400 companies on the X-axis. We can visually see how the intercepts become less varied as we are looking at the hierarchical model with different intercept and slopes. Model 1 Shrinkage: set.seed(84732) COMPANY_chains &lt;- Diff_inter_train %&gt;% spread_draws(`(Intercept)`, b[,COMPANY]) %&gt;% mutate(mu_j = `(Intercept)` + b) COMPANY_summary_scaled &lt;- COMPANY_chains %&gt;% select(-`(Intercept)`, -b) %&gt;% mean_qi(.width = 0.80) %&gt;% mutate(COMPANY = fct_reorder(COMPANY, mu_j)) ggplot( sample_n(COMPANY_summary_scaled,70), aes(x = COMPANY, y = mu_j, ymin = .lower, ymax = .upper)) + geom_pointrange() + geom_hline(yintercept = mean(data$Earnings_next_year_Scaled), linetype = &quot;dashed&quot;) + xaxis_text(angle = 90, hjust = 1) Model 2 Shrinkage: set.seed(84732) COMPANY_chains &lt;- Diff_inter_slope_train %&gt;% spread_draws(`(Intercept)`, b[,COMPANY]) %&gt;% mutate(mu_j = `(Intercept)` + b) COMPANY_summary_scaled &lt;- COMPANY_chains %&gt;% select(-`(Intercept)`, -b) %&gt;% mean_qi(.width = 0.80) %&gt;% mutate(COMPANY = fct_reorder(COMPANY, mu_j)) ggplot( sample_n(COMPANY_summary_scaled,70), aes(x = COMPANY, y = mu_j, ymin = .lower, ymax = .upper)) + geom_pointrange() + geom_hline(yintercept = mean(data$Earnings_next_year_Scaled), linetype = &quot;dashed&quot;) + xaxis_text(angle = 90, hjust = 1) 3.4.3 Interpreting Coefficents tidy(Diff_inter_train, effects = &quot;fixed&quot;, conf.int = TRUE, conf.level = .80) ## # A tibble: 13 × 5 ## term estimate std.error conf.low conf.high ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 0.320 0.0744 0.225 0.417 ## 2 EARNINGS_Scaled 0.222 0.00686 0.213 0.230 ## 3 EARNINGS_1_YEAR_AGO 0.0961 0.00613 0.0883 0.104 ## 4 SectorConsumer Discretionary 0.115 0.0856 0.00449 0.225 ## 5 SectorConsumer Staples 0.341 0.0998 0.212 0.469 ## 6 SectorEnergy 0.337 0.112 0.195 0.480 ## 7 SectorFinancials 0.288 0.0862 0.178 0.400 ## 8 SectorHealth Care 0.0608 0.0865 -0.0500 0.173 ## 9 SectorIndustrials 0.132 0.0843 0.0248 0.242 ## 10 SectorInformation Technology 0.0635 0.0844 -0.0449 0.174 ## 11 SectorMaterials 0.0971 0.0989 -0.0316 0.224 ## 12 SectorReal Estate -0.0825 0.0959 -0.207 0.0401 ## 13 SectorUtilities 0.234 0.0962 0.110 0.357 For the model Diff_inter_train, we could see that the earnings this year has a much higher impact on the prediction of next year’s earnings compared to the earnings 1 year ago (0.22158622 and 0.09614204). With the sector, we can clearly see that different sectors have completely different estimation for earnings. Based on the model, if a company is in Consumer Staples,Energy or Financials, that company will likely have higher prediction for earnings next year than the others. On the other hand, if a company is in the Real Estate field, that company will likely to have a lower earnings than the others. tidy(Diff_inter_slope_train, effects = &quot;fixed&quot;, conf.int = TRUE, conf.level = .80) ## # A tibble: 13 × 5 ## term estimate std.error conf.low conf.high ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 0.308 0.0555 0.236 0.378 ## 2 EARNINGS_Scaled 0.547 0.0203 0.521 0.573 ## 3 EARNINGS_1_YEAR_AGO 0.0408 0.00697 0.0320 0.0500 ## 4 SectorConsumer Discretionary -0.0272 0.0630 -0.107 0.0536 ## 5 SectorConsumer Staples 0.0583 0.0726 -0.0339 0.152 ## 6 SectorEnergy 0.181 0.0794 0.0778 0.283 ## 7 SectorFinancials 0.119 0.0623 0.0397 0.201 ## 8 SectorHealth Care 0.000288 0.0636 -0.0801 0.0823 ## 9 SectorIndustrials 0.0192 0.0619 -0.0592 0.0996 ## 10 SectorInformation Technology -0.0370 0.0613 -0.116 0.0419 ## 11 SectorMaterials -0.0381 0.0704 -0.128 0.0521 ## 12 SectorReal Estate -0.128 0.0697 -0.217 -0.0371 ## 13 SectorUtilities 0.0597 0.0685 -0.0272 0.148 For the Diff_inter_slope_train model, the impact of this year’s earnings is much stronger compared to the other variables (0.5467834739). It states that for one billion increase in earnings this year, the earnings for next year will increase by 0.5467 billions. Earnings next year also has much stronger impact than earnings 1 year ago. About the sector, the situation is also fairly similar as companies that are in Consumer Staples, Energy, or Financials will likely to have higher next year’s earnings than the others. However, in this model, the impact of the sector is much smaller than the Diff_inter_train model. The most notable negative earnings impact also comes from real estate as according to the model, company that is in real estate will see the earnings next year smaller than the baseline earnings of 0.12 billions. Global Standard Deviation Parameters tidy(Diff_inter_train, effects = &quot;ran_pars&quot;) ## # A tibble: 2 × 3 ## term group estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 sd_(Intercept).COMPANY COMPANY 0.312 ## 2 sd_Observation.Residual Residual 0.403 ((0.3116985^2))/((0.3116985^2 + 0.4029317^2))*100 ## [1] 37.43824 In the model with varying intercepts, about 37.43% of the variance can be explained between companies. tidy(Diff_inter_slope_train, effects = &quot;ran_pars&quot;) ## # A tibble: 4 × 3 ## term group estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 sd_(Intercept).COMPANY COMPANY 0.285 ## 2 sd_EARNINGS_Scaled.COMPANY COMPANY 0.316 ## 3 cor_(Intercept).EARNINGS_Scaled.COMPANY COMPANY -0.743 ## 4 sd_Observation.Residual Residual 0.353 The standard deviation \\(\\sigma_1\\) in the Earnings scaled coefficient (\\(\\beta_{1j}\\)) is likely to be around 0.31 billion per year. In the grand scheme of things, this number is quite high. For \\(\\sigma_y\\), an individual Company’s net earnings next year tend to deviate from their own mean model by 0.35 billion. There is a semi strong correlation between the Company Specific \\(\\beta_{0j}\\) and \\(\\beta_{1j}\\) parameters of -0.74. It seems that company’s with initial earnings will tend to experience a decrease in earnings compared to their previous years. "],["bayes-forecast.html", "Chapter 4 Bayes Forecast 4.1 Model Parameters 4.2 Modeling", " Chapter 4 Bayes Forecast library(bayesforecast) Bayesian Forecast SARIMA: In our last model specification, we decided to use the bayesforecast package. In particular we are fitting a SARIMA model in Stan. SARIMA stands for seasonal auto-regressive integrated moving average. This is an extension of ARIMA and is therefore more robust as it is able to support seasonal data. ARIMA is a method that combines both auto-regressive methods and moving averages- it is widely used on time series data in attempts to predict future values. There are four components that explain time series data, trend, seasonality, irregularity, and cyclic components. Reason to use Bayesian forecast: The reason that we are using Bayesian forecast is that it accounts for seasonal order, trends, or seasonality, which could help make the model more accurate. Moreover, Bayesian Forecast can also automate the process within the stan_sarima and auto_arima, in which the model will see which order, seasonal, or trend, fits with certain companies and variables. For Bayesian Forecast, we will first try to see the accuracy rate of the model compared to our complex hierarchical model. After that, using Bayesian Forecast, we will try to see what are the predicted earnings after ten years for Amazon (symbol: “AMZN”), Microsoft (symbol: “MSFT”), and American Airlines (symbol: “AAL”). 4.1 Model Parameters Here are the parameters for an ARIMA model: P - Order of the AR term. This is the number of Y to be used as predictors. For example, if we are predicting 2021 earnings, how many previous years earnings are we going to use? Q - Order of the MA term. This is the number of lagged forecast errors. How many past forecast errors will we be using? D - The minimum differencing period. A stationary time series implies one that has properties that do not depend on the time at which the series is observed. As mentioned above, SARIMA is able to support seasonal data. Below are the parameters for a SARIMA model that ARIMA does not have. P - Seasonal auto-regressive order. A P=1 would make use of the first seasonally offset observation in the model, e.g. t-(m1) or t-12. A P=2, would use the last two seasonally offset observations t-(m1), t-(m2). D - Seasonal difference order. A D of 1 would calculate a first order seasonal difference and a Q=1 would use a first order error in the model (e.g. moving average). Q - Seasonal moving average order. M - The number of time steps for a single seasonal period. M is a very important parameter as it influences the P, D, and Q parameters. For example, an m of 5 for yearly data suggests a 5-year seasonal cycle (in the context of business cycles. Model Tuning For the model we will use the default stan_sarima model in order to predict the earnings for the company. The reason for that is because for each company, it will have different order, seasonal and period. Manually picking the order and seasonal for each company might result in large errors as each company belongs in different fields, different sectors and also have different stories. Manually picking each company might result in a lot of inaccuracies. As we chose the default prior for the companies, it will assume that each company will have a prior normal function for the moving average and auto-regressive parameters. Additionally, the auto-regressive parameters and moving averages will have the same normal function of N(0,0.5) 4.2 Modeling 4.2.1 Accuracy In order to predict next year’s earnings for every company, as it acts like a time series model, we need to have companies that have at least 3 years of earnings. We first eliminate the companies that have only 2 years of earnings in this data set, then we will create a function to find the earnings for the next year of these companies. COMPANY_Names &lt;- data_elimatedO$COMPANY unique_COMPANY &lt;- unique(COMPANY_Names) unique_COMPANY data_elimatedO %&gt;% group_by(COMPANY) %&gt;% summarize(n=n()) %&gt;% filter(n &lt;= 2) unique_COMPANY_2 &lt;-unique_COMPANY unique_COMPANY_2 = unique_COMPANY_2[factor(unique_COMPANY_2) %in% c(&quot;ABBV&quot;,&quot;AMCR&quot; ,&quot;BAC&quot; ,&quot;CARR&quot;, &quot;CTVA&quot;, &quot;DOW&quot;, &quot;FB&quot;, &quot;GOOG&quot;, &quot;GOOGL&quot;, &quot;INTC&quot;, &quot;JNJ&quot;, &quot;OTIS&quot;, &quot;PSX&quot;, &quot;FOX&quot;, &quot;FOXA&quot;, &quot;HD&quot;, &quot;KHC&quot;, &quot;LYB&quot;, &quot;MDLZ&quot;, &quot;MRK&quot;, &quot;MRNA&quot;, &quot;OTIS&quot;, &quot;PEP&quot;, &quot;JPM&quot;) == FALSE] total_number_of_COMPANY &lt;- length(unique_COMPANY) total_number_of_COMPANY_2 &lt;- length(unique_COMPANY_2) After finding all of the desirable companies, we are left with 464 companies. We then created a function in order to predict the earnings next year for these 464 companies using Bayesian Forecast: point &lt;- high &lt;- low &lt;- rep(0, 464) for (i in 1: total_number_of_COMPANY_2) { company &lt;- unique_COMPANY_2[i] data_new &lt;- data_elimatedO %&gt;% filter(COMPANY == company) %&gt;% dplyr::select(EARNINGS_Scaled, YEAR) %&gt;% mutate(min_year = min(YEAR), max_year = max(YEAR)) %&gt;% arrange(EARNINGS_Scaled) min_year &lt;- data_new$min_year[1] max_year &lt;- data_new$max_year[1] vector &lt;- data_new$EARNINGS_Scaled myts &lt;- ts(vector, start=c(min_year), end=c(max_year), frequency=1) sf1 = stan_sarima(ts = myts, iter = 2 * 5000) result = forecast(object = sf1,h = 1) point[i] &lt;- result$mean low[i] &lt;- result$lower[2] high[i] &lt;- result$upper[2] } df &lt;- data.frame(unique_COMPANY_2[1: total_number_of_COMPANY_2], point, low, high) df After having the data set, we then combine the function with the actual data set in order to see the accuracy rate for these models in which it predicts the earnings next year for these companies. names(df) df &lt;- df %&gt;% mutate(COMPANY = `unique_COMPANY_2.1.total_number_of_COMPANY_2.`) %&gt;% select(- `unique_COMPANY_2.1.total_number_of_COMPANY_2.`) testing &lt;- data_elimatedO %&gt;% group_by(COMPANY) %&gt;% filter(row_number()==1) training &lt;- anti_join(data_elimatedO, testing) testing bayesian_forecast_earnigns = merge(x = df, y = testing, by = &quot;COMPANY&quot;, all.x = TRUE) bayesian_forecast_earnigns bayesian_forecast_earnigns &lt;- bayesian_forecast_earnigns %&gt;% select(COMPANY, point, low, high, YEAR, Earnings_next_year_Scaled) bayesian_forecast_earnigns write_csv(bayesian_forecast_earnigns, &quot;bayesian_forecast_earnigns.csv&quot;) bayesian_forecast_earnigns &lt;- read.csv(&quot;bayesian_forecast_earnigns.csv&quot;) After combining the two data sets, we then evaluate the accuracy rate for the Bayesian forecast model: bayesian_forecast_earnigns &lt;- bayesian_forecast_earnigns %&gt;% mutate(median_dist = abs(point - Earnings_next_year_Scaled)) %&gt;% mutate(Is_90 = (Earnings_next_year_Scaled &gt;= low) &amp; (Earnings_next_year_Scaled &lt;= high)) mean(bayesian_forecast_earnigns$median_dist) ## [1] 0.6589985 mean(bayesian_forecast_earnigns$Is_90) ## [1] 0.5150862 Overall, as we can see, the accuracy is not really high. The distance from the median is 0.6589, which shows that a company’s earnings will be off by around 0.6589 billions, which is a fairly high number. Moreover, only 51.5% of the companies is within the 90% percentile, which is a relatively small number. Overall, the model performs worse than the hierarchical model on section 3. 4.2.2 Company Predictions After creating the model, we will then move on to predict the earnings of the future year for the companies. The companies we are going to predict will be Amazon (symbol: “AMZN”), Microsoft (symbol: “MSFT”), and American Airlines (symbol: “AAL”). Amazon Prediction AMZN &lt;- data %&gt;% filter(COMPANY == &#39;AMZN&#39;) %&gt;% dplyr::select(EARNINGS_Scaled) %&gt;% arrange(EARNINGS_Scaled) vector &lt;- AMZN$EARNINGS_Scaled myts &lt;- ts(vector, start=c(1999), end=c(2021), frequency=1) sf1_Amazon = stan_sarima(ts = myts,order = c(1,1,1),seasonal = c(1,1,1), prior_mu0 = student(mu = 0,sd = 1,df = 7), iter = 2 * 5000) check_residuals(sf1_Amazon) autoplot(forecast(object = sf1_Amazon,h = 12)) First, for Amazon, we could see that the earnings are predicted to increase from 2021 to 2030, moving from 21 billions to around 38 billions in 2030. However, we could see that the prediction states that the earnings growth tends to slow down over time over the period. Looking at the expected values of the posterior predictive errors, prior to 2016, Amazon’s residuals seem to be consistently sitting at 0. A concern to this is how stagnant the residual line is, as a robust model will have a more varied fluctuation around 0. This may be the case that we only have around 20 observations. There is a general rule of thumb that for any ARIMA model, we should have at least 50, but preferably more than 100 observations to feed into the model. Otherwise we will not be able to pick up important features in our data (Box and Tiao 1975). Looking at the auto-correlation function, the most noticeable spike in the ACF plot is 2 years. The ACF plot tells us how correlated the present value is with past values or lagged values. This means that the most correlated value to the present value is 2 years afterwords. In this specific context, Amazon’s present day earnings in billions is most correlated with amazon’s earnings in billions two years ago. The information in the PACF plot, the partial auto-correlation function, does the same as the ACF plot but rather compares residual values instead of actual values. Microsoft Prediction MSFT &lt;- data %&gt;% filter(COMPANY == &#39;MSFT&#39;) %&gt;% dplyr::select(EARNINGS_Scaled) %&gt;% arrange(EARNINGS_Scaled) vector &lt;- MSFT$EARNINGS_Scaled myts &lt;- ts(vector, start=c(1999), end=c(2021), frequency=1) sf1_Microsofct = stan_sarima(ts = myts,iter = 2 * 5000) check_residuals(sf1_Microsofct) autoplot(forecast(object = sf1_Microsofct,h = 12)) Looking at data from Microsoft, the most correlated value with the present day value is 1 year behind. Both ACF and PACF are consistent with this. Since we only have around 20, observations, we will not have more fluctuation so our EV of PPE. American Airlines Prediction AAL &lt;- data %&gt;% filter(COMPANY == &#39;AAL&#39;) %&gt;% dplyr::select(EARNINGS_Scaled) %&gt;% arrange(EARNINGS_Scaled) vector &lt;- AAL$EARNINGS_Scaled myts &lt;- ts(vector, start=c(1999), end=c(2021), frequency=1) sf1_aal = stan_sarima(ts = myts,iter = 2 * 5000) check_residuals(sf1_aal) autoplot(forecast(object = sf1_aal,h = 12)) For American Airlines, we see a different story. As we can see from the graph, as airlines industry is an extremely cyclical field, the earnings fluctuate a lot. We could see that they fluctuate a lot during the previous years. With that in mind, the model predicts that American Airlines will not improve the much during the following years from 2021 to 2030 as predicting a cyclical company’s earnings could be a really difficult story. It did not show the similar pattern as Amazon. Looking at the ACF lab plot, the most noticeable increase is when we go from 1 difference (so looking at two year behind) to looking at 2 difference. This suggests that particularly for American Airlines, the difference parameter should be 2. "],["finalmodel.html", "Chapter 5 Final Model 5.1 Model Structure 5.2 Model Evaluation 5.3 Summary", " Chapter 5 Final Model From the models we explored, we have chosen the hierarchical model with different slopes and intercepts to be the best one. We provide a standalone summary of this model below. 5.1 Model Structure model_diff_inter_slope_train_data &lt;- training %&gt;% select(c(&quot;Earnings_next_year_Scaled&quot;,&quot;Sector&quot;,&quot;COMPANY&quot;,&quot;EARNINGS_Scaled&quot;,&quot;EARNINGS_1_YEAR_AGO&quot;)) %&gt;% na.omit() diff_slope_inter_model_train &lt;- stan_glmer( Earnings_next_year_Scaled ~ EARNINGS_Scaled + EARNINGS_1_YEAR_AGO + (EARNINGS_Scaled | COMPANY) + Sector, data = model_diff_inter_slope_train_data, family = gaussian, chains = 4, iter = 5000*2, seed = 84735, prior_PD = FALSE) write_rds(diff_slope_inter_model_train, &quot;Diff_inter_slope_train_nick.rds&quot;) Diff_inter_slope_train &lt;- readRDS(&quot;Diff_inter_slope_train_nick.rds&quot;) prior_summary(Diff_inter_slope_train) ## Priors for model &#39;Diff_inter_slope_train&#39; ## ------ ## Intercept (after predictors centered) ## Specified prior: ## ~ normal(location = 0.59, scale = 2.5) ## Adjusted prior: ## ~ normal(location = 0.59, scale = 1.5) ## ## Coefficients ## Specified prior: ## ~ normal(location = [0,0,0,...], scale = [2.5,2.5,2.5,...]) ## Adjusted prior: ## ~ normal(location = [0,0,0,...], scale = [1.74,1.60,4.60,...]) ## ## Auxiliary (sigma) ## Specified prior: ## ~ exponential(rate = 1) ## Adjusted prior: ## ~ exponential(rate = 1.6) ## ## Covariance ## ~ decov(reg. = 1, conc. = 1, shape = 1, scale = 1) ## ------ ## See help(&#39;prior_summary.stanreg&#39;) for more details Model Notation \\[\\begin{split} Y_{ij} | \\beta_{0j}, \\beta_{1j}, \\sigma_y &amp; \\sim N(\\mu_{ij}, \\sigma_y^2) \\;\\; \\text{ where } \\; \\mu_{ij} = \\beta_{0j} + \\beta_{1j} X_{ij} + \\beta_{2} X_{ij} + \\beta_{3} X_{ij}...\\\\ &amp; \\\\ \\beta_{0j} &amp; \\sim N(\\beta_0, \\sigma_0^2) \\\\ \\beta_{1j} &amp; \\sim N(\\beta_1, \\sigma_1^2) \\\\ &amp; \\\\ \\beta_{0c} &amp; \\sim N(0.59, 1.5^2) \\\\ \\beta_1 &amp; \\sim N(0, 1.74^2) \\\\ .\\\\ .\\\\ .\\\\ \\sigma_y &amp; \\sim \\text{Exp}(1.6) \\\\ \\sigma_0, \\sigma_1, ... &amp; \\sim \\text{(something a bit complicated)}. \\\\ \\end{split}\\] 5.2 Model Evaluation 5.2.1 Diagnostic Plots pp_check(Diff_inter_slope_train) The model did an okay job modeling the true structure of the data. We see that several company earnings’ on the right still seem to be causing the model some difficulties. 5.2.2 Metrics Diff_inter_slope_metrics ## MAE Within95 Within50 Within90 ## 1 0.2781092 0.7931034 0.4935345 0.7262931 In our model with varying intercepts and slopes the average median posterior prediction is off by 0.28 billion. Furthermore, 79.3% of the earnings next year fall within the 95% prediction intervals and 49.4% are within the 50% prediction intervals. tidy(Diff_inter_slope_train, effects = &quot;fixed&quot;, conf.int = TRUE, conf.level = .80) ## # A tibble: 13 × 5 ## term estimate std.error conf.low conf.high ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 0.308 0.0555 0.236 0.378 ## 2 EARNINGS_Scaled 0.547 0.0203 0.521 0.573 ## 3 EARNINGS_1_YEAR_AGO 0.0408 0.00697 0.0320 0.0500 ## 4 SectorConsumer Discretionary -0.0272 0.0630 -0.107 0.0536 ## 5 SectorConsumer Staples 0.0583 0.0726 -0.0339 0.152 ## 6 SectorEnergy 0.181 0.0794 0.0778 0.283 ## 7 SectorFinancials 0.119 0.0623 0.0397 0.201 ## 8 SectorHealth Care 0.000288 0.0636 -0.0801 0.0823 ## 9 SectorIndustrials 0.0192 0.0619 -0.0592 0.0996 ## 10 SectorInformation Technology -0.0370 0.0613 -0.116 0.0419 ## 11 SectorMaterials -0.0381 0.0704 -0.128 0.0521 ## 12 SectorReal Estate -0.128 0.0697 -0.217 -0.0371 ## 13 SectorUtilities 0.0597 0.0685 -0.0272 0.148 The impact of this year’s earnings is much stronger compared to the other variables (0.5467834739). It states that for one billion increase in earnings this year, the earnings for next year will increase by 0.5467 billions. Earnings next year also has much stronger impact than earnings 1 year ago. About the sector, the situation is also fairly similar as companies that are in Consumer Staples, Energy, or Financials will likely to have higher next year’s earnings than the others. However, in this model, the impact of the sector is smaller than the Diff_inter_train model. The most notable negative earnings impact also comes from real estate as according to the model, company that is in real estate will see the earnings next year smaller than the baseline earnings of 0.12 billions. tidy(Diff_inter_slope_train, effects = &quot;ran_pars&quot;) ## # A tibble: 4 × 3 ## term group estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 sd_(Intercept).COMPANY COMPANY 0.285 ## 2 sd_EARNINGS_Scaled.COMPANY COMPANY 0.316 ## 3 cor_(Intercept).EARNINGS_Scaled.COMPANY COMPANY -0.743 ## 4 sd_Observation.Residual Residual 0.353 The standard deviation \\(\\sigma_1\\) in the Earnings scaled coefficient (\\(\\beta_{1j}\\)) is likely to be around 0.31 billion per year. It refers that the standard deviation of the random slope for the Earnings coefficient is 0.31 billion dollars, which is a fairly high number. For \\(\\sigma_y\\), an individual Company’s net earnings next year tend to deviate from their own mean model by 0.35 billion. There is a semi strong correlation between the Company Specific \\(\\beta_{0j}\\) and \\(\\beta_{1j}\\) parameters of -0.74. It seems that company’s with initial earnings will tend to experience a decrease in earnings compared to their previous years. 5.3 Summary Our model demonstrates that future earnings are somewhat dependent on the specific company and their current and previous earnings. It also demonstrates that there are tons of other external factors that make earnings volatile from year to year, adding uncertainty into the model. The average posterior prediction for future earnings is 0.28 billion off from the true earnings that year. Furthermore, 79.3% of the earnings next year fall within the 95% prediction intervals and 49.4% are within the 50% prediction intervals. This model was the best of the ones we explored during this project, however its accuracy suffered due to the differences in companies and volatile earnings. Pros: Our pro of our model is that we are able to have companies gain information from other companies. We can also model and edit parameter information regarding earnings between companies and within companies. Cons: In our model, we assume earnings are indpendent of earnings in the past. However, bayes forcast is able to model soley based off past patterns and trends. "],["reflection.html", "Chapter 6 Reflection 6.1 Next Steps 6.2 Acknowledgements 6.3 Citations 6.4 Code Appendix", " Chapter 6 Reflection 6.1 Next Steps We would love to have more time to explore SARIMA models in bayesforecast, as well as improve our hierarchical models as well. With bayesforcast, this was a new package to us which took some time to learn, and there’s still more to explore that we weren’t able to get to. For our hierarchical models, they had some troubles dealing with outliers and the volatility of the financial world. We would like to explore more models in the future, possibly with more or different predictors, however these models took hours to run and a time constraint limited the amount of models we could try. Future models could become more accurate at predicting future earnings for companies. 6.2 Acknowledgements For our project, we want to thank Dr. Alicia Johnson for teaching us a lot of useful knowledge about Bayesian Statistics. Moreover, Dr. Alicia Johnson has given us a ton of feedback, and helped us through all the difficulties we have encountered. It has been a challenging project and we would not have been able to do it without the tools and techniques she taught us throughout this semester. 6.3 Citations “4.1 Seasonal Arima Models: Stat 510.” PennState: Statistics Online Courses, https://online.stat.psu.edu/stat510/lesson/4/4.1. “Bayesian Time Series Modeling with Stan [R Package Bayesforecast Version 1.0.1].” The Comprehensive R Archive Network, Comprehensive R Archive Network (CRAN), 17 June 2021, https://cran.r-project.org/web/packages/bayesforecast/index.html. “Forecasting: Principles and Practice (2nd Ed).” 8.9 Seasonal ARIMA Models, https://otexts.com/fpp2/seasonal-arima.html. Modeling Multiple Times Series with Applications - USGS. https://rmgsc.cr.usgs.gov/outgoing/threshold_articles/Tiao_Box1981.pdf. “Modeltime Integration.” Bayesmodels, https://albertoalmuinha.github.io/bayesmodels/articles/modeltime-integration.html. S&amp;P 500 Companies - S&amp;P 500 Index Components by Market Cap, https://www.slickcharts.com/sp500 “Yahoo Finance - Stock Market Live, Quotes, Business &amp; Finance News.” Yahoo! Finance, Yahoo!, https://finance.yahoo.com/. 6.4 Code Appendix To reproduce these results, the complete files and data are provided below: xfun::embed_files(c(&#39;data.Rmd&#39;, &#39;modeling.Rmd&#39;, &#39;finalmodel.Rmd&#39;, &#39;FINALDATASET.csv&#39;, &#39;RandomCompnay.csv&#39;, &#39;bayesian_forecast_earnigns.csv&#39;, &#39;Diff_Inter_train_nick.rds&#39;, &#39;Diff_inter_slope_train_nick.rds&#39;)) "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
