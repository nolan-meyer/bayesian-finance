[["index.html", "Bayesian Finance: Modeling Earnings for S&amp;P 500 Companies Chapter 1 Welcome 1.1 Introduction 1.2 Motivation", " Bayesian Finance: Modeling Earnings for S&amp;P 500 Companies Nicholas Di, Nolan Meyer, Duc Ngo Macalester College, Fall 2021, STAT 454 Capstone Project Chapter 1 Welcome 1.1 Introduction Add intro here… We can add these line thingys lol 1.2 Motivation Add motivations here… "],["data.html", "Chapter 2 Data 2.1 Data Source 2.2 Variables 2.3 Data Cleaning 2.4 Initial Explorations", " Chapter 2 Data 2.1 Data Source Our data includes financial information on companies in the S&amp;P 500 stock index from 1999-2021. This information was scraped from Yahoo Finance in November of 2021, and collected in a csv format for data analysis. To collect the data, first, we got the list of the current 500 S&amp;P companies from Slickchart (https://www.slickcharts.com/sp500). After having the list of the companies, we then moved on to the financials information of the listed companies on yahoo finance (https://finance.yahoo.com/) to get the metrics such as sales, earnings, cogs, stock price, and market sector. We then scraped the data using BeautifulSoup library in Python and turned that into a CSV file. After getting the data, our main goal is to analyze and model this data to better improve projections for a company’s future metrics, like earnings. 2.2 Variables The main variables that are used in this project are defined below: Variable Meaning YEAR The financial year of the company COMPANY The company’s stock abbreviation symbol MARKET.CAP The total market capitalization of the company (Volume * Price) EARNINGS The earnings in dollars for the previous year for the given company SALES How much the company sold in dollars last year CASH How much cash the company has in dollars at the end of the previous year Name The full name of the company Sector The name of the sector that the company is a part of Earnings_next_year The amount of money in dollars that the company earns in the following year 2.3 Data Cleaning 2.3.1 Data Pre-Processing First, as the earnings, cash and other variables are really large, we have decided to divide them by 1 billion. The reason for that is to make it easier to interpret and to understand the model. #Scaling Variables of Interest data &lt;- data %&gt;% mutate(EARNINGS_Scaled = EARNINGS/1000000000, CASH_Scaled = CASH/1000000000, MARKET.CAP_Scaled = MARKET.CAP/1000000000, Earnings_next_year_Scaled = Earnings_next_year/1000000000, SALES_Scaled = SALES/1000000000) Next, we added lagged variables for earnings and sales as we believe information from previous years will help predict future metrics. #Adding Lagged Variables data &lt;- data %&gt;% group_by(COMPANY) %&gt;% mutate(EARNINGS_1_YEAR_AGO = lead(EARNINGS_Scaled, n = 1), EARNINGS_2_YEAR_AGO = lead(EARNINGS_Scaled, n = 2), EARNINGS_3_YEAR_AGO = lead(EARNINGS_Scaled, n = 3), EARNINGS_4_YEAR_AGO = lead(EARNINGS_Scaled, n = 4)) %&gt;% mutate(SALES_1_YEAR_AGO = lead(SALES_Scaled, n = 1), SALES_2_YEAR_AGO = lead(SALES_Scaled, n = 2), SALES_3_YEAR_AGO = lead(SALES_Scaled, n = 3), SALES_4_YEAR_AGO = lead(SALES_Scaled, n = 4)) 2.3.2 Graphing Outliers We will do some more basic exploration regarding our data set, particularly our dependent variable of interest: Earnings the next year. Earnings_next_year_Scaled is the company’s earnings the next year. So Earnings_next_year_Scaled value of 1 for Company X in 2015 can be interpreted as 1 billion dollars in earnings in 2016. We explore Earnings next year below: #Graphing Outliers data %&gt;% ggplot(aes(x = Earnings_next_year_Scaled)) + geom_boxplot() data %&gt;% ggplot(aes(x = EARNINGS_Scaled, y= Earnings_next_year_Scaled))+ geom_point() There is a high number of outliers in our data, this is not ideal as we do not want to fit a model that includes outliers as predicting and modeling posterior distributions would be hard in the grand scheme. We removed outliers by classifying them as one if they are 1.5*IQR (Interquartile Range) Q &lt;- quantile(data$Earnings_next_year_Scaled, probs=c(.25, .75), na.rm = TRUE) iqr &lt;- IQR(data$Earnings_next_year_Scaled, na.rm = TRUE) up &lt;- Q[2]+1.5*iqr # Upper Range low&lt;- Q[1]-1.5*iqr # Lower Range eliminated&lt;- subset(data, data$Earnings_next_year_Scaled &gt; (Q[1] - 1.5*iqr) &amp; data$Earnings_next_year_Scaled &lt; (Q[2]+1.5*iqr)) data_elimatedO &lt;- eliminated We graph the data points without outliers below: data_elimatedO %&gt;% ggplot(aes(x = Earnings_next_year_Scaled)) + geom_boxplot() data_elimatedO %&gt;% ggplot(aes(x = EARNINGS_Scaled, y= Earnings_next_year_Scaled))+ geom_point() This is much better as the data points are closer in proximity. 2.4 Initial Explorations After collecting the data, we then looked for the distribution of the companies within the S&amp;P 500. The first plot we are going to create is the number of companies within the period: As we see above, we have data for about 70% of the companies in the S&amp;P 500 for the first year of our data, and by 2021 we have about every single company within the index. It is important for us to have data on as many companies as possible over this time period so that we can better capture trends and make more accurate models based on the data. Next, we investigated how market cap, specifically the sum of all the companies’ market caps, varied from year to year. By grouping by year, we were able to easily combine each companies market cap together to create the plot above. This plot highlights trends in the overall market, we see a general increase over time in market cap, with sharp decreases around 2008 and 2020. Those two years align with the housing market crash and COVID respectively, both which led to decreases in the stock market. By identifying trends in the overall market, we may have a better idea about how individual companies may perform. Our main objective with this project is to be able to accurately predict future earnings using metrics like sales, previous earnings, and other variables like the sector of the company. We found that overall, among the top 50 companies (based on market cap), there were positive relationships between earning and sales. This relationship varies based on the market sector, with IT having the most positive relationship, and Consumer Staples having the least positive relationship. This indicates to us that both sector and sales may be important predictors of earnings that we should explore using in our future models. For most sectors, it appears that the farther back we go, the flatter the relationship between Earnings and past earnings is. If we plot earnings next year with earnings four years ago, we will see that almost all sectors have different slopes. "],["modeling.html", "Chapter 3 Modeling 3.1 Set Up 3.2 Model 1 3.3 Model 1 w/ Interaction 3.4 Model 2", " Chapter 3 Modeling 3.1 Set Up Since we have time-series data, we create a testing set by subseting each company’s 2nd to latest year. We will predict earnings’s next year with the testing set and compare it with the actual values to help determine model accuracy. testing &lt;- data_elimatedO %&gt;% group_by(COMPANY) %&gt;% filter(row_number()==1) training &lt;- anti_join(data_elimatedO, testing) 3.2 Model 1 3.2.1 Model Structure The first model is a regular simple normal regression, as we are not taking advantage of the grouped structure of our data (by company). We have a complete pooled regression here. model_1_no_data &lt;- eliminated %&gt;% select(c(&quot;Earnings_next_year_Scaled&quot;,&quot;EARNINGS_Scaled&quot;,&quot;Is_top_100&quot;)) %&gt;% na.omit() model_1_no &lt;- stan_glm( Earnings_next_year_Scaled ~ EARNINGS_Scaled, data = model_1_no_data, family = gaussian, prior_PD = FALSE, chains = 4, iter = 5000*2, seed = 84735, refresh = 0) Below is our first model notation with adjusted priors: prior_summary(model_1_no) \\[\\begin{split} Y_i | \\beta_0, \\beta_1, \\sigma &amp; \\stackrel{ind}{\\sim} N(\\mu_i, \\sigma^2) \\;\\; \\text{ where } \\mu_i = \\beta_0 + \\beta_1 X_i\\\\ \\beta_{0c} &amp; \\sim N(0.68, 2^2) \\\\ \\beta_1 &amp; \\sim N(0, 1.6^2) \\\\ \\sigma &amp; \\sim \\text{Exp}(1.3) \\\\ \\end{split}\\] 3.2.2 Model Evaluation pp_check(model_1_no) Our model seems to do decent, however the structure of the data is still a bit right skewed as some of the companies included in our dataset have higher earnings around the 3+ billion range. 3.3 Model 1 w/ Interaction 3.3.1 Model Structure Next, we interact sector with earnings hoping to improve the model’s fit to our dependent variable. model_1_interact_no_data &lt;- data_elimatedO %&gt;% select(c(&quot;Earnings_next_year_Scaled&quot;,&quot;EARNINGS_Scaled&quot;,&quot;Sector&quot;,&quot;Is_top_100&quot;)) %&gt;% na.omit() model_1_interact_no &lt;- stan_glm( Earnings_next_year_Scaled ~ EARNINGS_Scaled*Sector, data = model_1_interact_no_data, family = gaussian, prior_PD = FALSE, chains = 4, iter = 5000*2, seed = 84735, refresh = 0) 3.3.2 Model Evaluation pp_check(model_1_interact_no) Unfortunately, this model performs similarly to the previous and is not the best fit. 3.4 Model 2 3.4.1 Model Structure Now, we will move on to utilizing the structure of our data, where we have consecutive observations for each company for several years. Note: we fit each model twice, one using all data and another using training data (for evaluation purposes). model_2 &lt;- read_rds(&quot;model_2_Heirc_1yr_no_o_nick.rds&quot;) model_2_train &lt;- readRDS(&quot;model_2_Heirc_1yr_no_o_train_nick.rds&quot;) model_2_Heirc_1yr_data &lt;- data_elimatedO %&gt;% select(c(&quot;Earnings_next_year_Scaled&quot;,&quot;EARNINGS_Scaled&quot;,&quot;COMPANY&quot;)) %&gt;% na.omit() model_2_Heirc_1yr_no_o &lt;- stan_glmer( Earnings_next_year_Scaled ~ EARNINGS_Scaled + (1 | COMPANY), data = model_2_Heirc_1yr_data, family = gaussian, chains = 4, iter = 5000*2, seed = 84735, prior_PD = FALSE) model_2_Heirc_1yr_training_data &lt;- training %&gt;% select(c(&quot;Earnings_next_year_Scaled&quot;,&quot;EARNINGS_Scaled&quot;,&quot;COMPANY&quot;)) %&gt;% na.omit() model_2_Heirc_1yr_no_o_train &lt;- stan_glmer( Earnings_next_year_Scaled ~ EARNINGS_Scaled + (1 | COMPANY), data = model_2_Heirc_1yr_training_data, family = gaussian, chains = 4, iter = 5000*2, seed = 84735, prior_PD = FALSE, refresh = 0) prior_summary(model_2) Below is the notation for our model 2: \\[\\begin{split} \\text{Relationship within company:} &amp; \\\\ Y_{ij} | \\beta_{0j}, \\beta_1, \\sigma_y &amp; \\sim N(\\mu_{ij}, \\sigma_y^2) \\;\\; \\text{ where } \\mu_{ij} = \\beta_{0j} + \\beta_1 X_{ij} \\\\ &amp; \\\\ \\text{Variability between companies:} &amp; \\\\ \\beta_{0j} &amp; \\stackrel{ind}{\\sim} N(\\beta_0, \\sigma_0^2) \\\\ &amp; \\\\ \\text{Prior information on Globals with Adjusted Prior} &amp; \\\\ \\beta_{0c} &amp; \\sim N(0.68, 2^2) \\\\ \\beta_1 &amp; \\sim N(0, 1.6^2) \\\\ \\sigma_y &amp; \\sim \\text{Exp}(1.3) \\\\ \\sigma_0 &amp; \\sim \\text{Exp}(1) \\\\ \\end{split}\\] "],["finalmodel.html", "Chapter 4 Final Model", " Chapter 4 Final Model "],["reflection.html", "Chapter 5 Reflection 5.1 Next Steps 5.2 Acknowledgements 5.3 Citations 5.4 Code Appendix", " Chapter 5 Reflection 5.1 Next Steps 5.2 Acknowledgements 5.3 Citations 5.4 Code Appendix "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
