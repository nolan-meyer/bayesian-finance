[["index.html", "Bayesian Finance: Modeling Earnings for S&amp;P 500 Companies Chapter 1 Welcome 1.1 Introduction", " Bayesian Finance: Modeling Earnings for S&amp;P 500 Companies Nicholas Di, Nolan Meyer, Duc Ngo Macalester College, Fall 2021, STAT 454 Capstone Project Chapter 1 Welcome 1.1 Introduction Add intro here…(intro, motivations, outline of modeling approach) "],["data.html", "Chapter 2 Data 2.1 Data Source 2.2 Variables 2.3 Data Cleaning 2.4 Initial Explorations", " Chapter 2 Data 2.1 Data Source Our data includes financial information on companies in the S&amp;P 500 stock index from 1999-2021. This information was scraped from Yahoo Finance in November of 2021, and collected in a csv format for data analysis. To collect the data, first, we got the list of the current 500 S&amp;P companies from Slickchart (https://www.slickcharts.com/sp500). After having the list of the companies, we then moved on to the financials information of the listed companies on yahoo finance (https://finance.yahoo.com/) to get the metrics such as sales, earnings, cogs, stock price, and market sector. We then scraped the data using BeautifulSoup library in Python and turned that into a CSV file. After getting the data, our main goal is to analyze and model this data to better improve projections for a company’s future metrics, like earnings. 2.2 Variables The main variables that are used in this project are defined below: Variable Meaning YEAR The financial year of the company COMPANY The company’s stock abbreviation symbol MARKET.CAP The total market capitalization of the company (Volume * Price) EARNINGS The earnings in dollars for the previous year for the given company SALES How much the company sold in dollars last year CASH How much cash the company has in dollars at the end of the previous year Name The full name of the company Sector The name of the sector that the company is a part of Earnings_next_year The amount of money in dollars that the company earns in the following year 2.3 Data Cleaning 2.3.1 Data Pre-Processing First, as the earnings, cash and other variables are really large, we have decided to divide them by 1 billion. The reason for that is to make it easier to interpret and to understand the model. #Scaling Variables of Interest data &lt;- data %&gt;% mutate(EARNINGS_Scaled = EARNINGS/1000000000, CASH_Scaled = CASH/1000000000, MARKET.CAP_Scaled = MARKET.CAP/1000000000, Earnings_next_year_Scaled = Earnings_next_year/1000000000, SALES_Scaled = SALES/1000000000) Next, we added lagged variables for earnings and sales as we believe information from previous years will help predict future metrics. #Adding Lagged Variables data &lt;- data %&gt;% group_by(COMPANY) %&gt;% mutate(EARNINGS_1_YEAR_AGO = lead(EARNINGS_Scaled, n = 1), EARNINGS_2_YEAR_AGO = lead(EARNINGS_Scaled, n = 2), EARNINGS_3_YEAR_AGO = lead(EARNINGS_Scaled, n = 3), EARNINGS_4_YEAR_AGO = lead(EARNINGS_Scaled, n = 4)) %&gt;% mutate(SALES_1_YEAR_AGO = lead(SALES_Scaled, n = 1), SALES_2_YEAR_AGO = lead(SALES_Scaled, n = 2), SALES_3_YEAR_AGO = lead(SALES_Scaled, n = 3), SALES_4_YEAR_AGO = lead(SALES_Scaled, n = 4)) 2.3.2 Graphing Outliers We will do some more basic exploration regarding our data set, particularly our dependent variable of interest: Earnings the next year. Earnings_next_year_Scaled is the company’s earnings the next year. So Earnings_next_year_Scaled value of 1 for Company X in 2015 can be interpreted as 1 billion dollars in earnings in 2016. We explore Earnings next year below: #Graphing Outliers data %&gt;% ggplot(aes(x = Earnings_next_year_Scaled)) + geom_boxplot() data %&gt;% ggplot(aes(x = EARNINGS_Scaled, y= Earnings_next_year_Scaled))+ geom_point() There is a high number of outliers in our data, this is not ideal as we do not want to fit a model that includes outliers as predicting and modeling posterior distributions would be hard in the grand scheme. We removed outliers by classifying them as one if they are outside the range of +/- (0.9 * IQR) (Interquartile Range). Q &lt;- quantile(data$Earnings_next_year_Scaled, probs=c(.25, .75), na.rm = TRUE) iqr &lt;- IQR(data$Earnings_next_year_Scaled, na.rm = TRUE) up &lt;- Q[2]+.90*iqr # Upper Range low&lt;- Q[1]-.90*iqr # Lower Range eliminated&lt;- subset(data, data$Earnings_next_year_Scaled &gt; (low) &amp; data$Earnings_next_year_Scaled &lt; (up)) data_elimatedO &lt;- eliminated We graph the data points without outliers below: data_elimatedO %&gt;% ggplot(aes(x = Earnings_next_year_Scaled)) + geom_boxplot() data_elimatedO %&gt;% ggplot(aes(x = EARNINGS_Scaled, y= Earnings_next_year_Scaled))+ geom_point() This is much better as the data points are closer in proximity. 2.4 Initial Explorations After collecting the data, we then looked for the distribution of the companies within the S&amp;P 500. The first plot we are going to create is the number of companies within the period: As we see above, we have data for about 70% of the companies in the S&amp;P 500 for the first year of our data, and by 2021 we have about every single company within the index. It is important for us to have data on as many companies as possible over this time period so that we can better capture trends and make more accurate models based on the data. Next, we investigated how market cap, specifically the sum of all the companies’ market caps, varied from year to year. By grouping by year, we were able to easily combine each companies market cap together to create the plot above. This plot highlights trends in the overall market, we see a general increase over time in market cap, with sharp decreases around 2008 and 2020. Those two years align with the housing market crash and COVID respectively, both which led to decreases in the stock market. By identifying trends in the overall market, we may have a better idea about how individual companies may perform. Our main objective with this project is to be able to accurately predict future earnings using metrics like sales, previous earnings, and other variables like the sector of the company. We found that overall, among the top 50 companies (based on market cap), there were positive relationships between earning and sales. This relationship varies based on the market sector, with IT having the most positive relationship, and Consumer Staples having the least positive relationship. This indicates to us that both sector and sales may be important predictors of earnings that we should explore using in our future models. For most sectors, it appears that the farther back we go, the flatter the relationship between Earnings and past earnings is. If we plot earnings next year with earnings four years ago, we will see that almost all sectors have different slopes. "],["hierarchical-modeling.html", "Chapter 3 Hierarchical Modeling 3.1 Set Up 3.2 Model 2 3.3 Model 3 3.4 Model Evaluations", " Chapter 3 Hierarchical Modeling 3.1 Set Up Since we have time-series data, we created a testing set by sub-setting each company’s 2nd to latest year. We will predict earnings next year with the testing set and compare it with the actual values to help determine model accuracy. testing &lt;- data_elimatedO %&gt;% group_by(COMPANY) %&gt;% filter(row_number()==1) training &lt;- anti_join(data_elimatedO, testing) 3.2 Model 2 3.2.1 Model Structure Now, we will move on to utilizing the structure of our data, where we have consecutive observations for each company for several years. Note: we fit each model twice, one using all data and another using training data (for evaluation purposes). model_2 &lt;- read_rds(&quot;model_2_Heirc_1yr_no_o_nick.rds&quot;) model_2_train &lt;- readRDS(&quot;model_2_Heirc_1yr_no_o_train_nick.rds&quot;) model_2_Heirc_1yr_data &lt;- data_elimatedO %&gt;% select(c(&quot;Earnings_next_year_Scaled&quot;,&quot;EARNINGS_Scaled&quot;,&quot;COMPANY&quot;)) %&gt;% na.omit() model_2_Heirc_1yr_no_o &lt;- stan_glmer( Earnings_next_year_Scaled ~ EARNINGS_Scaled + (1 | COMPANY), data = model_2_Heirc_1yr_data, family = gaussian, chains = 4, iter = 5000*2, seed = 84735, prior_PD = FALSE) model_2_Heirc_1yr_training_data &lt;- training %&gt;% select(c(&quot;Earnings_next_year_Scaled&quot;,&quot;EARNINGS_Scaled&quot;,&quot;COMPANY&quot;)) %&gt;% na.omit() model_2_Heirc_1yr_no_o_train &lt;- stan_glmer( Earnings_next_year_Scaled ~ EARNINGS_Scaled + (1 | COMPANY), data = model_2_Heirc_1yr_training_data, family = gaussian, chains = 4, iter = 5000*2, seed = 84735, prior_PD = FALSE, refresh = 0) prior_summary(model_2) Below is the notation for our model 2: \\[\\begin{split} \\text{Relationship within company:} &amp; \\\\ Y_{ij} | \\beta_{0j}, \\beta_1, \\sigma_y &amp; \\sim N(\\mu_{ij}, \\sigma_y^2) \\;\\; \\text{ where } \\mu_{ij} = \\beta_{0j} + \\beta_1 X_{ij} \\\\ &amp; \\\\ \\text{Variability between companies:} &amp; \\\\ \\beta_{0j} &amp; \\stackrel{ind}{\\sim} N(\\beta_0, \\sigma_0^2) \\\\ &amp; \\\\ \\text{Prior information on Globals with Adjusted Prior} &amp; \\\\ \\beta_{0c} &amp; \\sim N(0.68, 2^2) \\\\ \\beta_1 &amp; \\sim N(0, 1.6^2) \\\\ \\sigma_y &amp; \\sim \\text{Exp}(1.3) \\\\ \\sigma_0 &amp; \\sim \\text{Exp}(1) \\\\ \\end{split}\\] 3.3 Model 3 3.3.1 Model Structure In addition to having a hierarchical regression with one predictor, we fit another model using 3 years of lagged data as well as a sector dummy variable. We do this because earnings current year seemed to have a differential effect on earnings next year dependent on the company’s sector. Again, we fit two different models. model_3 &lt;- readRDS(&quot;model_complex_with_hiar_nick.rds&quot;) model_3_train &lt;- readRDS(&quot;model_complex_with_hiar_train_nick.rds&quot;) model_complex_with_hiar_data &lt;- data_elimatedO %&gt;% select(c(&quot;Earnings_next_year_Scaled&quot;,&quot;EARNINGS_Scaled&quot;,&quot;EARNINGS_1_YEAR_AGO&quot;,&quot;EARNINGS_2_YEAR_AGO&quot;,&quot;EARNINGS_3_YEAR_AGO&quot;,&quot;COMPANY&quot;,&quot;Sector&quot;)) %&gt;% na.omit() model_complex_with_hiar &lt;- stan_glmer( Earnings_next_year_Scaled ~ EARNINGS_Scaled + EARNINGS_1_YEAR_AGO + EARNINGS_2_YEAR_AGO + EARNINGS_3_YEAR_AGO + (1 | COMPANY) + Sector, data = model_complex_with_hiar_data, family = gaussian, chains = 4, iter = 5000*2, seed = 84735, prior_PD = FALSE, refresh = 0) model_complex_with_hiar_data_train &lt;- training %&gt;% select(c(&quot;Earnings_next_year_Scaled&quot;,&quot;EARNINGS_Scaled&quot;,&quot;EARNINGS_1_YEAR_AGO&quot;,&quot;EARNINGS_2_YEAR_AGO&quot;,&quot;EARNINGS_3_YEAR_AGO&quot;,&quot;COMPANY&quot;,&quot;Sector&quot;)) %&gt;% na.omit() model_complex_with_hiar_train &lt;- stan_glmer( Earnings_next_year_Scaled ~ EARNINGS_Scaled + EARNINGS_1_YEAR_AGO + EARNINGS_2_YEAR_AGO + EARNINGS_3_YEAR_AGO + (1 | COMPANY) + Sector, data = model_complex_with_hiar_data_train, family = gaussian, chains = 4, iter = 5000*2, seed = 84735, prior_PD = FALSE) prior_summary(model_3) Model 3 Notation: \\[\\begin{split} \\text{Relationship within company:} &amp; \\\\ Y_{ij} | \\beta_{0j}, \\beta_1, \\sigma_y &amp; \\sim N(\\mu_{ij}, \\sigma_y^2) \\;\\; \\text{ where } \\mu_{ij} = \\beta_{0j} + \\beta_1 X_{ij} + \\beta_2 X_{ij} + \\beta_2 X_{ij}...\\\\ &amp; \\\\ \\text{Variability between companies:} &amp; \\\\ \\beta_{0j} &amp; \\stackrel{ind}{\\sim} N(\\beta_0, \\sigma_0^2) \\\\ &amp; \\\\ \\text{Prior information on Globals with Adjusted Prior} &amp; \\\\ \\beta_{0c} &amp; \\sim N(0.72, 2^2) \\\\ \\beta_1 &amp; \\sim N(0, 1.63^2) \\\\ \\beta_2 &amp; \\sim N(0, 1.63^2) \\\\ \\beta_3 &amp; \\sim N(0, 1.57^2) \\\\ .\\\\ .\\\\ .\\\\ \\sigma_y &amp; \\sim \\text{Exp}(1.2) \\\\ \\sigma_0 &amp; \\sim \\text{Exp}(1) \\\\ \\end{split}\\] 3.4 Model Evaluations 3.4.1 Is this the right model? Next we plot the posterior distributions and compare to the actual values observed in the data set. pp_check(model_2) pp_check(model_3) Again, several company earnings’s on the right seem to be causing model fitness difficulties. Both models run into this problem. There seems to be no difference between model 2 and 3 in terms of fitting the structure of earnings next year. We will now dive into predicting the accuracy of the models. 3.4.2 How Accurate are the models? Model 2 - Specific Examples with companies: Below, we compare our predictions for American Airlines. We plot 750 random values predicted from our predictions (out of 20,000). As we can see below our predictions range cover the actual value of earnings for 2020 fiscal year for both models. data %&gt;% filter(COMPANY == test_comp) %&gt;% filter(YEAR &lt;= most_recent_year+1) %&gt;% ggplot(aes(x= YEAR, y=EARNINGS_Scaled))+ geom_point() + geom_line() + geom_segment(data = graphing_predictions, aes(x = most_recent_year, xend = most_recent_year+1, y = actual, yend = EARNINGS_Scaled), alpha = 0.03, colour = &quot;red&quot;) # mcmc_areas(predict_next_year, prob = 0.8) + # ggplot2::scale_y_discrete(labels = c(`test_comp`)) + geom_vline(xintercept = actual, linetype = &quot;dashed&quot;, colour = &quot;red&quot;) #Need Help Because X scale is different Model 3 - Specific Examples with companies: data %&gt;% filter(COMPANY == test_comp) %&gt;% filter(YEAR &lt;= most_recent_year) %&gt;% ggplot(aes(x= YEAR, y=EARNINGS_Scaled))+ geom_point() + geom_line() + geom_segment(data = graphing_predictions, aes(x = most_recent_year - 1, xend = most_recent_year, y = actual, yend = EARNINGS_Scaled), alpha = 0.03, colour = &quot;red&quot;) # mcmc_areas(predict_next_year, prob = 0.8) + # ggplot2::scale_y_discrete(labels = c(`test_comp`)) + geom_vline(xintercept = actual, linetype = &quot;dashed&quot;, colour = &quot;red&quot;) The predictions for the model_3 seem to be slightly better as more posterior predictive points are near the actual value for “2020”. Evaluating Metrics Model 2: model_2_metrics ## MAE Within95 Within50 ## 1 0.368009 0.8275154 0.4887064 Model 3: model_3_metrics ## MAE Within95 Within50 ## 1 0.337252 0.8362069 0.5258621 We can see that our model 3 preforms slightly better. Where our average median posterior prediction is off by .3697147 billion as opposed to model 2’s .40 billion. Furthermore, our 95% and 50% interval values are both slightly better than model 2’s. Shrinkage Since we modeled based off different companies having different intercepts, it is worthwhile to checkout how the company baselines shrunk compared to each other and between the two different models. Model 2 Shrinkage: ggplot( sample_n(COMPANY_summary_scaled,70), aes(x = COMPANY, y = mu_j, ymin = .lower, ymax = .upper)) + geom_pointrange() + geom_hline(yintercept = mean(data$Earnings_next_year_Scaled), linetype = &quot;dashed&quot;) + xaxis_text(angle = 90, hjust = 1) Model 3 Shrinkage: ggplot( sample_n(COMPANY_summary_scaled,70), aes(x = COMPANY, y = mu_j, ymin = .lower, ymax = .upper)) + geom_pointrange() + geom_hline(yintercept = mean(data$Earnings_next_year_Scaled), linetype = &quot;dashed&quot;) + xaxis_text(angle = 90, hjust = 1) Global Standard Deviation Parameters tidy(model_2, effects = &quot;ran_pars&quot;) ## # A tibble: 2 × 3 ## term group estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 sd_(Intercept).COMPANY COMPANY 0.451 ## 2 sd_Observation.Residual Residual 0.552 (0.4512^2)/((0.4512^2) + (0.5519447^2)) ## [1] 0.4005739 tidy(model_3, effects = &quot;ran_pars&quot;) ## # A tibble: 2 × 3 ## term group estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 sd_(Intercept).COMPANY COMPANY 0.429 ## 2 sd_Observation.Residual Residual 0.545 (0.4294^2)/((0.4294^2) + (0.5450631)^2) ## [1] 0.3829548 In model 2, about 40.06% of the variance can be explained between companies. In model 3, about 38.29% of the variance can be explained between companies. This means that when including sectors and lagged variables, more variable shrinkage occurred. Unfortunately this shrinkage is not as evident in the plotted graphs above. Model 3’s baseline intercepts differ among each other slightly less than model 2’s. "],["bayes-forecast.html", "Chapter 4 Bayes Forecast 4.1 Model Parameters 4.2 Modeling", " Chapter 4 Bayes Forecast library(bayesforecast) Bayesian Forecast SARIMA: In our last model specification, we decided to use the bayesforecast package. In particular we are fitting a SARIMA model in Stan. SARIMA stands for seasonal auto-regressive integrated moving average. This is an extension of ARIMA and is therefore more robust as it is able to support seasonal data. ARIMA is a method that combines both auto-regressive methods and moving averages- it is widely used on time series data in attempts to predict future values. There are four components that explain time series data, trend, seasonality, irregularity, and cyclic components. 4.1 Model Parameters Here are the parameters for an ARIMA model: P - Order of the AR term. This is the number of Y to be used as predictors. For example, if we are predicting 2021 earnings, how many previous years earnings are we going to use? Q - Order of the MA term. This is the number of lagged forecast errors. How many past forecast errors will we be using? D - The minimum differencing period. A stationary time series implies one that has properties that do not depend on the time at which the series is observed. As mentioned above, SARIMA is able to support seasonal data. Below are the parameters for a SARIMA model that ARIMA does not have. P - Seasonal autoregressive order. A P=1 would make use of the first seasonally offset observation in the model, e.g. t-(m1) or t-12. A P=2, would use the last two seasonally offset observations t-(m1), t-(m2). D - Seasonal difference order. A D of 1 would calculate a first order seasonal difference and a Q=1 would use a first order error in the model (e.g. moving average). Q - Seasonal moving average order. M - The number of time steps for a single seasonal period. M is a very important parameter as it influences the P, D, and Q parameters. For example, an m of 5 for yearly data suggests a 5-year seasonal cycle (in the context of business cycles. 4.2 Modeling After running the model, we will then move on to predict the earnings of the future year for the companies. The companies we are going to predict will be Amazon (symbol: “AMZN”), American Airlines (symbol: “AAL”), and Ford (symbol: “F”). AMZN &lt;- data %&gt;% filter(COMPANY == &#39;AMZN&#39;) %&gt;% dplyr::select(EARNINGS_Scaled) %&gt;% arrange(EARNINGS_Scaled) vector &lt;- AMZN$EARNINGS_Scaled myts &lt;- ts(vector, start=c(1999), end=c(2021), frequency=1) sf1 = stan_sarima(ts = myts,order = c(1,1,1),seasonal = c(1,1,1), prior_mu0 = student(mu = 0,sd = 1,df = 7)) sf1 ## ## y ~ Sarima(1,1,1) ## 23 observations and 1 dimension ## Differences: 1 seasonal Differences: 0 ## Current observations: 22 ## ## mean se 5% 95% ess Rhat ## mu0 0.6122 0.0074 -0.0389 1.4521 3946.370 1.0002 ## sigma0 2.2608 0.0055 1.7567 2.8966 3880.739 0.9999 ## ar 0.4249 0.0068 -0.4297 0.9536 3504.369 1.0006 ## ma -0.1344 0.0046 -0.5678 0.3905 3646.405 1.0012 ## loglik -50.2659 0.0246 -52.8948 -47.7647 3635.604 1.0000 ## ## Samples were drawn using sampling(NUTS). For each parameter, ess ## is the effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). check_residuals(sf1) autoplot(forecast(object = sf1,h = 12)) First, for Amazon, we could see that the earnings are predicted to increase from 2021 to 2030, moving from 21 billions to around 38 billions in 2030. For the residuals plot, we can see that the residuals expect to increase in the future and keep increasing as the farther from 2020, the data becomes more and more unpredictable. Overall, the residuals lie around -1 and 0, which suggests that the error is not that far off from the actual value. We then move on to the other companies: Microsoft and American Airlines: AAL &lt;- data %&gt;% filter(COMPANY == &#39;AAL&#39;) %&gt;% dplyr::select(EARNINGS_Scaled) %&gt;% arrange(EARNINGS_Scaled) vector &lt;- AAL$EARNINGS_Scaled myts &lt;- ts(vector, start=c(1999), end=c(2021), frequency=1) sf1 = stan_sarima(ts = myts,order = c(1,1,1),seasonal = c(1,1,1), prior_mu0 = student(mu = 0,sd = 1,df = 7)) check_residuals(sf1) autoplot(forecast(object = sf1,h = 12)) For American Airlines, we see a different story. As we can see from the graph, as airlines industry is an extremely cyclical field, the earnings fluctuate a lot. We could see that they fluctuate a lot during the previous years. With that in mind, the model predicts that American Airlines will not improve the much during the following years from 2021 to 2030. F &lt;- data %&gt;% filter(COMPANY == &#39;F&#39;) %&gt;% dplyr::select(EARNINGS_Scaled) %&gt;% arrange(EARNINGS_Scaled) vector &lt;- F$EARNINGS_Scaled myts &lt;- ts(vector, start=c(1999), end=c(2021), frequency=1) sf1 = stan_sarima(ts = myts,order = c(1,1,1),seasonal = c(1,1,1), prior_mu0 = student(mu = 0,sd = 1,df = 7)) check_residuals(sf1) autoplot(forecast(object = sf1,h = 12)) For Ford, it follows the same case as American Airlines. Even though the earnings increase, the model predicts that Ford’s earning will stay relatively the same throughout the year from 2021 to 2030. For the residuals part, the residuals fluctuate around -1 to 1, with some exceptions of 6 and 11 in around 2002 and 2020 (the year when COVID-19 started). "],["finalmodel.html", "Chapter 5 Final Model", " Chapter 5 Final Model "],["reflection.html", "Chapter 6 Reflection 6.1 Next Steps 6.2 Acknowledgements 6.3 Citations 6.4 Code Appendix", " Chapter 6 Reflection 6.1 Next Steps 6.2 Acknowledgements 6.3 Citations 6.4 Code Appendix "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
