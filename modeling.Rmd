# Hierarchical Modeling

```{r, echo=FALSE}
library(tidyverse)         # for reading in data, graphing, and cleaning
library(tidymodels)        # for modeling ... tidily
library(usemodels)         # for suggesting step_XXX() functions
library(glmnet)            # for regularized regression, including LASSO
library(naniar)            # for examining missing values (NAs)
library(lubridate)         # for date  manipulation
library(moderndive)        # for King County housing data
library(vip)               # for variable importance plots
library(rmarkdown)         # for paged tables
library(dplyr)
library(janitor)
library(ggplot2)
library(ggpubr)
library(dplyr)
library(bayesforecast)    # Bayes Forecasting (Playing around) 
library(bayesrules)
library(tidyverse)
library(rstanarm)
library(broom.mixed)
library(tidybayes)
library(tidyverse)
library(bayesplot)
library(ggplot2)
```

```{r, echo=FALSE}
data <- read.csv("FINALDATASET.csv")
#Selecting necessary variables
data <- dplyr::select(data, c("YEAR", "COMPANY", "MARKET.CAP", "EARNINGS", "SALES", "CASH", "Name", "Sector", "Earnings_next_year"))

#Scaling Variables of Interest
data <- data %>% 
  mutate(EARNINGS_Scaled = EARNINGS/1000000000,
         CASH_Scaled = CASH/1000000000,
         MARKET.CAP_Scaled = MARKET.CAP/1000000000,
         Earnings_next_year_Scaled = Earnings_next_year/1000000000,
         SALES_Scaled = SALES/1000000000)

#Adding Lagged Variables
data <- data %>% 
  group_by(COMPANY) %>%
  mutate(EARNINGS_1_YEAR_AGO = lead(EARNINGS_Scaled, n = 1), 
         EARNINGS_2_YEAR_AGO = lead(EARNINGS_Scaled, n = 2),
         EARNINGS_3_YEAR_AGO = lead(EARNINGS_Scaled, n = 3),
         EARNINGS_4_YEAR_AGO = lead(EARNINGS_Scaled, n = 4)) %>% 
  mutate(SALES_1_YEAR_AGO = lead(SALES_Scaled, n = 1),
         SALES_2_YEAR_AGO = lead(SALES_Scaled, n = 2),
         SALES_3_YEAR_AGO = lead(SALES_Scaled, n = 3),
         SALES_4_YEAR_AGO = lead(SALES_Scaled, n = 4))

#Add_Dummy if Company is 100
temp <- data %>% 
  group_by(COMPANY) %>% 
  summarize(count = n(), 
            mean_MC = mean(MARKET.CAP)) %>% 
  filter(count == 23) %>% 
  arrange(desc(mean_MC)) %>% 
  head(100)

temp <- temp$COMPANY

data <- data %>% 
  mutate(Is_top_100 = case_when(COMPANY %in% temp ~ 1,TRUE ~ 0))
```


## Set Up

**Creating a testing set**

Since we have time-series data, we created a testing set by sub-setting each company's 2nd to latest year. We left off the most recent year a company reports their earnings and created a testing set with this data, the training set includes all the other previous years. We will predict the earnings next year for each company with the testing set and compare it with the actual values to help determine model accuracy. 

```{r, include=FALSE}
Q <- quantile(data$Earnings_next_year_Scaled, probs=c(.25, .75), na.rm = TRUE)
iqr <- IQR(data$Earnings_next_year_Scaled, na.rm = TRUE)
up <-  Q[2]+.90*iqr # Upper Range  
low<- Q[1]-.90*iqr # Lower Range
eliminated<- subset(data, data$Earnings_next_year_Scaled > (low) & data$Earnings_next_year_Scaled < (up))
data_elimatedO <- eliminated
```

```{r}
testing <- data_elimatedO %>% 
   group_by(COMPANY) %>% 
   filter(row_number()==1)

training <- anti_join(data_elimatedO, testing)
```

```{r, include=FALSE}
predict_model<- function(Company, Model){
  test_comp <- Company
  test_1 <- testing %>% 
    filter(COMPANY == test_comp)
  predict_next_year <- posterior_predict(
  Model, 
  newdata = data.frame(test_1))
  most_recent_year <- testing %>% 
   filter(COMPANY == test_comp) %>% 
   filter(row_number()==1)
most_recent_year <- most_recent_year$YEAR
actual <- data %>% 
  filter(COMPANY == test_comp & YEAR == most_recent_year-1) %>% 
  select(c("EARNINGS_Scaled","YEAR"))
actual <- actual$EARNINGS_Scaled
randomsample <- sample_n(as.data.frame(predict_next_year), 750)
graphing_predictions <- randomsample %>% 
  mutate(EARNINGS_Scaled = `1`,
         YEAR = most_recent_year) %>% 
  add_row(EARNINGS_Scaled = actual, YEAR = most_recent_year -1) %>% 
  select(c("EARNINGS_Scaled","YEAR"))
g <- data %>% 
  filter(COMPANY == test_comp) %>% 
  filter(YEAR <= most_recent_year) %>% 
  ggplot(aes(x= YEAR, y=EARNINGS_Scaled))+
  geom_point() +
  geom_line() +
  geom_segment(data = graphing_predictions, aes(x = most_recent_year - 1, 
                   xend = most_recent_year,
                   y = actual, 
                   yend = EARNINGS_Scaled),
               alpha = 0.03,
               colour = "red")
  return(g)
}

collect_metrics_pred <- function(Model, Datametric){
  predictions_mode_2 <- posterior_predict(
  Model, 
  newdata = data.frame(Datametric))
prediction_dataframe <- as.data.frame(predictions_mode_2)

temp <- prediction_dataframe %>% 
bind_rows(summarise(.,across(where(is.numeric),median),
                    across(where(is.character),~"Median")))
meadian_predictions <- tail(temp, 1)
meadian_predictions <- t(meadian_predictions)
meadian_predictions<-as.data.frame(meadian_predictions)
meadian_predictions <- meadian_predictions %>% 
  mutate(median = `20001`) %>% 
  select(median)

temp <- prediction_dataframe %>% 
bind_rows(summarise(.,across(where(is.numeric),quantile, .025),
                    across(where(is.character),~"Lower95")))
predictions_lower <- tail(temp, 1)
predictions_lower <- t(predictions_lower)
predictions_lower<-as.data.frame(predictions_lower)
predictions_lower95 <- predictions_lower %>% 
  mutate(lower95 = `20001`) %>% 
  select(lower95)

temp <- prediction_dataframe %>% 
bind_rows(summarise(.,across(where(is.numeric),quantile, .25),
                    across(where(is.character),~"Lower50")))
predictions_lower <- tail(temp, 1)
predictions_lower <- t(predictions_lower)
predictions_lower<-as.data.frame(predictions_lower)
predictions_lower50 <- predictions_lower %>% 
  mutate(lower50 = `20001`) %>% 
  select(lower50)

temp <- prediction_dataframe %>% 
bind_rows(summarise(.,across(where(is.numeric),quantile, .975),
                    across(where(is.character),~"Upper95")))
predictions_upper <- tail(temp, 1)
predictions_upper <- t(predictions_upper)
predictions_upper<-as.data.frame(predictions_upper)
predictions_upper95 <- predictions_upper %>% 
  mutate(upper95 = `20001`) %>% 
  select(upper95)

temp <- prediction_dataframe %>% 
bind_rows(summarise(.,across(where(is.numeric),quantile, .75),
                    across(where(is.character),~"Upper50")))
predictions_upper <- tail(temp, 1)
predictions_upper <- t(predictions_upper)
predictions_upper<-as.data.frame(predictions_upper)
predictions_upper50 <- predictions_upper %>% 
  mutate(upper50 = `20001`) %>% 
  select(upper50)

temp <- prediction_dataframe %>% 
bind_rows(summarise(.,across(where(is.numeric),quantile, .05),
                    across(where(is.character),~"Lower90")))
predictions_upper <- tail(temp, 1)
predictions_upper <- t(predictions_upper)
predictions_upper<-as.data.frame(predictions_upper)
predictions_Lower90 <- predictions_upper %>% 
  mutate(Lower90 = `20001`) %>% 
  select(Lower90)

temp <- prediction_dataframe %>% 
bind_rows(summarise(.,across(where(is.numeric),quantile, .95),
                    across(where(is.character),~"Upper90")))
predictions_upper <- tail(temp, 1)
predictions_upper <- t(predictions_upper)
predictions_upper<-as.data.frame(predictions_upper)
predictions_Upper90 <- predictions_upper %>% 
  mutate(Upper90 = `20001`) %>% 
  select(Upper90)

Testing_with_metrics <- cbind(Datametric, predictions_upper95, predictions_lower95,predictions_upper50,predictions_lower50, predictions_Lower90, predictions_Upper90, meadian_predictions)

median_error <- Testing_with_metrics %>% 
  mutate(absDist = abs(median - Earnings_next_year_Scaled))

in_95 <- Testing_with_metrics %>% 
  mutate(Is_95 = (Earnings_next_year_Scaled < upper95) & (Earnings_next_year_Scaled > lower95))

in_50 <- Testing_with_metrics %>% 
  mutate(Is_50 = (Earnings_next_year_Scaled < upper50) & (Earnings_next_year_Scaled > lower50))

in_90 <- Testing_with_metrics %>% 
  mutate(Is_90 = (Earnings_next_year_Scaled < Upper90) & (Earnings_next_year_Scaled > Lower90))

med <- median(median_error$absDist)
mean95 <- mean(in_95$Is_95)
mean50 <- mean(in_50$Is_50)
mean90 <- mean(in_90$Is_90)
model_2_metrics <- matrix(c(med,mean95,mean50, mean90), ncol = 4)
colnames(model_2_metrics) <- c("MAE","Within95","Within50", "Within90")
model_2_metrics <- as.data.frame(model_2_metrics)
return(model_2_metrics)
}
```


**Tuning Priors**

Earnings, like most things in the financial markets are quite volatile, making them harder to model. We do not have a strong understanding of the variability between and within companies, thus we are using weakly informative priors. This means that the models will rely more on the data to form the posterior distributions. We are using the default prior values found in the stan_glmer package. 


**Defining Notation**

In our model notation below, $Y_{ij}$ is earnings for the next year after the $i$th year for company $j$ and $X_{ij}$ is the earnings in the $i$th year for company $j$. For example, Y = Earnings of Apple in 2017 in billions, and X = Earnings of Apple in 2016 in billions. 



## Model 1: Hierarchical w/ Different Intercepts

### Model Structure

Now, we will move on to utilizing the structure of our data, where we have consecutive observations for each company for several years. We start with a model with varying intercepts. Note: we fit each model using training data for evaluation purposes.

```{r}
Diff_inter_train <- readRDS("Diff_inter_train_nick.rds")
```

```{r, eval=FALSE}
model_diff_inter_train_data <- training %>% 
  select(c("Earnings_next_year_Scaled","EARNINGS_Scaled","EARNINGS_1_YEAR_AGO","COMPANY","Sector")) %>% na.omit()

model_diff_inter_train <- stan_glmer(
  Earnings_next_year_Scaled ~ EARNINGS_Scaled + EARNINGS_1_YEAR_AGO  + Sector + (1 | COMPANY) , data = model_diff_inter_train_data, 
  family = gaussian,
  chains = 4, iter = 5000*2, seed = 84735, 
  prior_PD = FALSE, refresh = 0)

write_rds(model_diff_inter_train, "Diff_Inter_train_nick.rds")
```

```{r, eval=FALSE}
prior_summary(Diff_inter_train)
```

Below is the notation for the differing intercepts model:

$$\begin{split}
\text{Relationship within company:} & \\
Y_{ij} | \beta_{0j}, \beta_1, \sigma_y 
& \sim N(\mu_{ij}, \sigma_y^2) \;\; \text{ where } \mu_{ij} = \beta_{0j} + \beta_1 X_{ij} + \beta_2 X_{ij} + \beta_3 X_{ij}...\\
& \\
\text{Variability between companies:} & \\
\beta_{0j} & \stackrel{ind}{\sim} N(\beta_0, \sigma_0^2) \\
& \\
\text{Prior information on Globals with Adjusted Prior} & \\
\beta_{0c} & \sim N(0.59, 1.5^2) \\
\beta_1 & \sim N(0, 1.74^2) \\
\beta_2 & \sim N(0, 1.60^2) \\
\beta_3 & \sim N(0, 4.60^2) \\
.\\
.\\
.\\
\sigma_y  & \sim \text{Exp}(1.6) \\
\sigma_0  & \sim \text{Exp}(1) \\
\end{split}$$



## Model 2: Hierarchical w/ Different Slopes and Intercepts

### Model Structure

In addition to having a hierarchical regression with different intercepts, we decided to add a model with different intercepts and slopes.  

**Rational behind different slopes:**

Below we graph 4 random companies, we can see that earnings in the current year impacts earnings next year differently among different companies. 

```{r}
eliminated %>% 
  filter(COMPANY %in% c("AAL","CVS","DAL","WAB")) %>% 
  ggplot(., aes(x = EARNINGS_Scaled, y = Earnings_next_year_Scaled)) + 
    geom_point() + 
    geom_smooth(method = "lm", se = FALSE) + 
    facet_grid(~ COMPANY)
```

To get a better idea of the varying slopes, we graphed 50 random companies together. 

```{r}
vector <- eliminated$COMPANY
vector <- sample_n(as.data.frame(vector), 50)
vector <- as.list(vector)
eliminated %>% 
  filter(COMPANY %in% vector$vector) %>% 
  ggplot(aes(x=EARNINGS_Scaled, y= Earnings_next_year_Scaled, group = COMPANY))+
  geom_smooth(method = "lm", se= FALSE, size = 0.5)
```

Instead of using a global earnings slope coefficient, we decide to add a hierarchical layer so that company slope coefficients can learn from each other and yet have their own specific slope. If we have a universal slope coefficient, there will be a great deal of bias within our model.  


```{r}
Diff_inter_slope_train <- readRDS("Diff_inter_slope_train_nick.rds")
```

```{r, eval=FALSE}
model_diff_inter_slope_train_data <- training %>%
  select(c("Earnings_next_year_Scaled","Sector","COMPANY","EARNINGS_Scaled","EARNINGS_1_YEAR_AGO")) %>% 
  na.omit()

diff_slope_inter_model_train <- stan_glmer(
  Earnings_next_year_Scaled ~ EARNINGS_Scaled + EARNINGS_1_YEAR_AGO + (EARNINGS_Scaled | COMPANY) + Sector, data = model_diff_inter_slope_train_data, 
  family = gaussian,
  chains = 4, iter = 5000*2, seed = 84735, 
  prior_PD = FALSE)
write_rds(diff_slope_inter_model_train, "Diff_inter_slope_train_nick.rds")
```

```{r, eval=FALSE}
prior_summary(Diff_inter_slope_train)
```

Different intercepts & slopes model notation: 

$$\begin{split}
Y_{ij} | \beta_{0j}, \beta_{1j}, \sigma_y & \sim N(\mu_{ij}, \sigma_y^2) \;\; \text{ where } \; \mu_{ij} = \beta_{0j} + \beta_{1j} X_{ij} + \beta_{2} X_{ij}  + \beta_{3} X_{ij}...\\
& \\
\beta_{0j} & \sim N(\beta_0, \sigma_0^2) \\
\beta_{1j} & \sim N(\beta_1, \sigma_1^2) \\
& \\
\beta_{0c} & \sim N(0.59, 1.5^2)  \\
\beta_1 & \sim N(0, 1.74^2) \\
.\\
.\\
.\\
\sigma_y & \sim \text{Exp}(1.6)    \\
\sigma_0, \sigma_1, ... & \sim \text{(something a bit complicated)}. \\
\end{split}$$


## Model Evaluations

### Is this the right model? 

Using our models we simulate replicated data and then compare these to the observed data to look for discrepancies between the two in the plots below.


```{r}
pp_check(Diff_inter_train) 
pp_check(Diff_inter_slope_train)
```

Several company earningsâ€™ on the right seem to be causing model fitness difficulties. Both models run into this problem, however the second model is able to capture more of the outliers on the right. 

Prior to removing the extreme outliers in our data set, the posterior predictive check for the models were worse as it left majority of the outliers uncovered. After filtering out for outliers, we evidently still have problems with both models.

Next we will examine the predictive accuracy of the models. 


### How Accurate are the models? 

Below, we compare our predictions for American Airlines, Delta Airlines, and Amazon. We plot 750 random values predicted from our predictions (out of 20,000) for both models. We can see below our predictions for each of these companies for both hierarchical models.

**Model 1 - Specific Examples with companies:**

```{r}
set.seed(84732)
predict_model("AAL", Diff_inter_train)
```

```{r}
set.seed(84732)
predict_model("DAL", Diff_inter_train)
```

```{r}
set.seed(84732)
predict_model("AMZN", Diff_inter_train)
```


**Model 2 - Specific Examples with companies:**

```{r}
set.seed(84732)
predict_model("AAL", Diff_inter_slope_train)
```

```{r}
set.seed(84732)
predict_model("DAL", Diff_inter_slope_train)
```

```{r}
set.seed(84732)
predict_model("AMZN", Diff_inter_slope_train)
```


The predictions for the second model (different intercepts and slopes) seem to be slightly better as more posterior predictive points are nearer to the actual values of earnings in "2020".

Notice how for both models, predictions for Delta Air Lines are completely off from the actual data. However, in our model with different intercepts and slopes, more predictions are closer to the actual value, thus having the model mean being closer as well.


```{r, include=FALSE}
set.seed(84732)
testing_1 <- testing %>% 
  na.omit()
Diff_inter_metrics <- collect_metrics_pred(Diff_inter_train, testing_1)
```


```{r, include=FALSE}
set.seed(84732)
testing_1 <- testing %>%
  na.omit()
Diff_inter_slope_metrics <- collect_metrics_pred(Diff_inter_slope_train, testing_1)
```


**Evaluating Metrics**

For the prediction metrics, using the training data set, we calculated 20,000 values for each individual company for their earnings next year (the last year we have data on them) using the hierarchical models. We then take the median value of the 20,000 for each company and calculate the mean distance from the median to the actual earnings observed for that year. We also computed the 95% and 50% prediction intervals by calculating the percentage of 20,000 predicted values are within the 2.5th and 97.5th percentile and 25 to 75th percentile respectively.  

Different Intercepts Model:

```{r}
Diff_inter_metrics
```

Different Intercepts & Slopes Model:

```{r}
Diff_inter_slope_metrics
```

We can see that our model with varying intercepts and slopes (model 2) preforms slightly better. Where our average median posterior prediction is off by 0.28 billion as opposed to 0.362 billion when we only have differing intercepts. Furthermore, our 95 and 50 interval values are both  better in the model with different intercept and slope.


**Shrinkage **

Since we modeled based off different companies having different intercepts, it is worthwhile to checkout how the company baselines shrunk compared to each other and between the two different models. We randomly sampled 70 companies, since if we plot all companies we will have more than 400 companies on the X-axis. We can visually see how the intercepts become less varied as we are looking at the hierarchical model with different intercept and slopes. 

Model 1 Shrinkage:

```{r}
set.seed(84732)
COMPANY_chains <- Diff_inter_train %>%
  spread_draws(`(Intercept)`, b[,COMPANY]) %>%
  mutate(mu_j = `(Intercept)` + b)
COMPANY_summary_scaled <- COMPANY_chains %>%
  select(-`(Intercept)`, -b) %>%
  mean_qi(.width = 0.80) %>%
  mutate(COMPANY = fct_reorder(COMPANY, mu_j))
ggplot(
    sample_n(COMPANY_summary_scaled,70),
    aes(x = COMPANY, y = mu_j, ymin = .lower, ymax = .upper)) +
    geom_pointrange() +
    geom_hline(yintercept = mean(data$Earnings_next_year_Scaled), linetype = "dashed") + 
  xaxis_text(angle = 90, hjust = 1)
```


Model 2 Shrinkage:

```{r}
set.seed(84732)
COMPANY_chains <- Diff_inter_slope_train %>%
  spread_draws(`(Intercept)`, b[,COMPANY]) %>%
  mutate(mu_j = `(Intercept)` + b)
COMPANY_summary_scaled <- COMPANY_chains %>%
  select(-`(Intercept)`, -b) %>%
  mean_qi(.width = 0.80) %>%
  mutate(COMPANY = fct_reorder(COMPANY, mu_j))
ggplot(
    sample_n(COMPANY_summary_scaled,70),
    aes(x = COMPANY, y = mu_j, ymin = .lower, ymax = .upper)) +
    geom_pointrange() +
    geom_hline(yintercept = mean(data$Earnings_next_year_Scaled), linetype = "dashed") + 
  xaxis_text(angle = 90, hjust = 1)
```


### Interpreting Coefficents

```{r}
tidy(Diff_inter_train, effects = "fixed", conf.int = TRUE, conf.level = .80)
```

For the model Diff_inter_train, we could see that the earnings this year has a much higher impact on the prediction of next year's earnings compared to the earnings 1 year ago (0.22158622 and 0.09614204). With the sector, we can clearly see that different sectors have completely different estimation for earnings. Based on the model, if a company is in Consumer Staples,Energy or Financials, that company will likely have higher prediction for earnings next year than the others. On the other hand, if a company is in the Real Estate field, that company will likely to have a lower earnings than the others.

```{r}
tidy(Diff_inter_slope_train, effects = "fixed", conf.int = TRUE, conf.level = .80)
```

For the Diff_inter_slope_train model, the impact of this year's earnings is much stronger compared to the other variables (0.5467834739). It states that for one billion increase in earnings this year, the earnings for next year will increase by 0.5467 billions. Earnings next year also has much stronger impact than earnings 1 year ago. 

About the sector, the situation is also fairly similar as companies that are in Consumer Staples, Energy, or Financials will likely to have higher next year's earnings than the others. However, in this model, the impact of the sector is much smaller than the Diff_inter_train model. The most notable negative earnings impact also comes from real estate as according to the model, company that is in real estate will see the earnings next year smaller than the baseline earnings of 0.12 billions. 


**Global Standard Deviation Parameters**

```{r}
tidy(Diff_inter_train, effects = "ran_pars")
((0.3116985^2))/((0.3116985^2 + 0.4029317^2))*100
```

In the model with varying intercepts, about 37.43% of the variance can be explained between companies. 


```{r}
tidy(Diff_inter_slope_train, effects = "ran_pars")
```

The standard deviation $\sigma_1$ in the Earnings scaled coefficient ($\beta_{1j}$) is likely to be around 0.31 billion per year. In the grand scheme of things, this number is quite high. 

For $\sigma_y$, an individual Company's net earnings next year tend to deviate from their own mean model by 0.35 billion. 

There is a semi strong correlation between the Company Specific $\beta_{0j}$ and $\beta_{1j}$ parameters of -0.74. It seems that company's with initial earnings will tend to experience a decrease in earnings compared to their previous years. 



# Bayes Forecast

```{r}
library(bayesforecast)
```

**Bayesian Forecast SARIMA:**

In our last model specification, we decided to use the bayesforecast package. In particular we are fitting a SARIMA model in Stan.

SARIMA stands for seasonal auto-regressive integrated moving average. This is an extension of ARIMA and is therefore more robust as it is able to support seasonal data. 

ARIMA is a method that combines both auto-regressive methods and moving averages- it is widely used on time series data in attempts to predict future values. There are four components that explain time series data, trend, seasonality, irregularity, and cyclic components. 


**Reason to use Bayesian forecast:**

The reason that we are using Bayesian forecast is that it accounts for seasonal order, trends, or seasonality, which could help make the model more accurate. Moreover, Bayesian Forecast can also automate the process within the stan_sarima and auto_arima, in which the model will see which order, seasonal, or trend, fits with certain companies and variables. 

For Bayesian Forecast, we will first try to see the accuracy rate of the model compared to our complex hierarchical model. After that, using Bayesian Forecast, we will try to see what are the predicted earnings after ten years for Amazon (symbol: "AMZN"), Microsoft (symbol: "MSFT"), and American Airlines (symbol: "AAL"). 


## Model Parameters

Here are the parameters for an ARIMA model: 

P - Order of the AR term. This is the number of Y to be used as predictors. For example, if we are predicting 2021 earnings, how many previous years earnings are we going to use? 

Q - Order of the MA term. This is the number of lagged forecast errors. How many past forecast errors will we be using? 

D - The minimum differencing period. A stationary time series implies one that has properties that do not depend on the time at which the series is observed. 


As mentioned above, SARIMA is able to support seasonal data. Below are the parameters for a SARIMA model that ARIMA does not have. 


P - Seasonal auto-regressive order. A P=1 would make use of the first seasonally offset observation in the model, e.g. t-(m1) or t-12. A P=2, would use the last two seasonally offset observations t-(m1), t-(m2).

D - Seasonal difference order. A D of 1 would calculate a first order seasonal difference and a Q=1 would use a first order error in the model (e.g. moving average).

Q - Seasonal moving average order.

M - The number of time steps for a single seasonal period. M is a very important parameter as it influences the P, D, and Q parameters. For example, an m of 5 for yearly data suggests a 5-year seasonal cycle (in the context of business cycles. 


**Model Tuning**

For the model we will use the default stan_sarima model in order to predict the earnings for the company. The reason for that is because for each company, it will have different order, seasonal and period. Manually picking the order and seasonal for each company might result in large errors as each company belongs in different fields, different sectors and also have different stories. Manually picking each company might result in a lot of inaccuracies. 

As we chose the default prior for the companies, it will assume that each company will have a prior normal function for the moving average and auto-regressive parameters. Additionally, the auto-regressive parameters and moving averages will have the same normal function of N(0,0.5)


## Modeling

### Accuracy

In order to predict next year's earnings for every company, as it acts like a time series model, we need to have companies that have at least 3 years of earnings. We first eliminate the companies that have only 2 years of earnings in this data set, then we will create a function to find the earnings for the next year of these companies.

```{r, eval=FALSE, include=FALSE}
## Original data set: 
Q <- quantile(data$Earnings_next_year_Scaled, probs=c(.25, .75), na.rm = TRUE)
iqr <- IQR(data$Earnings_next_year_Scaled, na.rm = TRUE)
up <-  Q[2]+.90*iqr # Upper Range  
low<- Q[1]-.90*iqr # Lower Range
eliminated<- subset(data, data$Earnings_next_year_Scaled > (low) & data$Earnings_next_year_Scaled < (up))
data_elimatedO <- eliminated
```

```{r, eval=FALSE}
COMPANY_Names <- data_elimatedO$COMPANY
unique_COMPANY <- unique(COMPANY_Names)

unique_COMPANY

data_elimatedO %>% 
  group_by(COMPANY) %>% 
  summarize(n=n()) %>% 
  filter(n <= 2)
```

```{r, eval=FALSE}
unique_COMPANY_2 <-unique_COMPANY

unique_COMPANY_2 = unique_COMPANY_2[factor(unique_COMPANY_2) %in% c("ABBV","AMCR" ,"BAC"  ,"CARR", "CTVA",  "DOW",   "FB", "GOOG",  "GOOGL", "INTC",  "JNJ", "OTIS", "PSX", "FOX", "FOXA", "HD", "KHC", "LYB", "MDLZ", "MRK", "MRNA", "OTIS", "PEP", "JPM") == FALSE] 

total_number_of_COMPANY <- length(unique_COMPANY)
total_number_of_COMPANY_2 <- length(unique_COMPANY_2)
```

```{r, eval=FALSE, include=FALSE}
total_number_of_COMPANY
total_number_of_COMPANY_2
```

After finding all of the desirable companies, we are left with 464 companies. We then created a function in order to predict the earnings next year for these 464 companies using Bayesian Forecast: 

```{r, eval=FALSE, echo=TRUE}
point <- high <- low <- rep(0, 464)

for (i in 1: total_number_of_COMPANY_2) {
  company <- unique_COMPANY_2[i]
  
  data_new <- data_elimatedO %>% 
    filter(COMPANY == company) %>% 
    dplyr::select(EARNINGS_Scaled, YEAR) %>% 
    mutate(min_year = min(YEAR), 
         max_year = max(YEAR)) %>% 
    arrange(EARNINGS_Scaled)
  
  min_year <- data_new$min_year[1]
  max_year <- data_new$max_year[1]

  
  vector <- data_new$EARNINGS_Scaled

  myts <- ts(vector, start=c(min_year), end=c(max_year), frequency=1)

  sf1 = stan_sarima(ts = myts, iter = 2 * 5000)
  
  result = forecast(object = sf1,h = 1)
  
  point[i] <- result$mean
  low[i] <- result$lower[2]
  high[i] <- result$upper[2]
}

df <- data.frame(unique_COMPANY_2[1: total_number_of_COMPANY_2], point, low, high)
df
```

After having the data set, we then combine the function with the actual data set in order to see the accuracy rate for these models in which it predicts the earnings next year for these companies. 

```{r, eval=FALSE, echo=TRUE}
names(df) 

df <- df %>% 
  mutate(COMPANY = `unique_COMPANY_2.1.total_number_of_COMPANY_2.`) %>% 
  select(- `unique_COMPANY_2.1.total_number_of_COMPANY_2.`)

testing <- data_elimatedO %>% 
   group_by(COMPANY) %>% 
   filter(row_number()==1)

training <- anti_join(data_elimatedO, testing)

testing

bayesian_forecast_earnigns = merge(x = df, y = testing, by = "COMPANY", all.x = TRUE)

bayesian_forecast_earnigns

bayesian_forecast_earnigns <- bayesian_forecast_earnigns %>% 
  select(COMPANY, point, low, high, YEAR, Earnings_next_year_Scaled)

bayesian_forecast_earnigns

write_csv(bayesian_forecast_earnigns, "bayesian_forecast_earnigns.csv")
```

```{r}
bayesian_forecast_earnigns <- read.csv("bayesian_forecast_earnigns.csv")
```


After combining the two data sets, we then evaluate the accuracy rate for the Bayesian forecast model:

```{r}
bayesian_forecast_earnigns <- bayesian_forecast_earnigns %>% 
  mutate(median_dist = abs(point - Earnings_next_year_Scaled)) %>% 
  mutate(Is_90 = (Earnings_next_year_Scaled >= low) & (Earnings_next_year_Scaled <= high))

mean(bayesian_forecast_earnigns$median_dist)
mean(bayesian_forecast_earnigns$Is_90)
```

Overall, as we can see, the accuracy is not really high. The distance from the median is 0.6589, which shows that a company's earnings will be off by around 0.6589 billions, which is a fairly high number. Moreover, only 51.5% of the companies is within the 90% percentile, which is a relatively small number. Overall, the model performs worse than the hierarchical model on section 3.



### Company Predictions

After creating the model, we will then move on to predict the earnings of the future year for the companies. The companies we are going to predict will be Amazon (symbol: "AMZN"), Microsoft (symbol: "MSFT"), and American Airlines (symbol: "AAL").

**Amazon Prediction**

```{r, results='hide'}
AMZN <- data %>% 
  filter(COMPANY == 'AMZN') %>% 
  dplyr::select(EARNINGS_Scaled) %>% 
  arrange(EARNINGS_Scaled)

vector <- AMZN$EARNINGS_Scaled

myts <- ts(vector, start=c(1999), end=c(2021), frequency=1)

sf1_Amazon = stan_sarima(ts = myts,order = c(1,1,1),seasonal = c(1,1,1),
                  prior_mu0 = student(mu = 0,sd = 1,df = 7), iter = 2 * 5000)
```

```{r}
check_residuals(sf1_Amazon)
autoplot(forecast(object = sf1_Amazon,h = 12))
```

First, for Amazon, we could see that the earnings are predicted to increase from 2021 to 2030, moving from 21 billions to around 38 billions in 2030. However, we could see that the prediction states that the earnings growth tends to slow down over time over the period. 

Looking at the expected values of the posterior predictive errors, prior to 2016, Amazon's residuals seem to be consistently sitting at 0. A concern to this is how stagnant the residual line is, as a robust model will have a more varied fluctuation around 0. This may be the case that we only have around 20 observations. There is a general rule of thumb that for any ARIMA model, we should have at least 50, but preferably more than 100 observations to feed into the model. Otherwise we will not be able to pick up important features in our data (Box and Tiao 1975).

Looking at the auto-correlation function, the most noticeable spike in the ACF plot is 2 years. The ACF plot tells us how correlated the present value is with past values or lagged values. This means that the most correlated value to the present value is 2 years afterwords. In this specific context, Amazon's present day earnings in billions is most correlated with amazon's earnings in billions two years ago. 

The information in the PACF plot, the partial auto-correlation function, does the same as the ACF plot but rather compares residual values instead of actual values. 


**Microsoft Prediction**

```{r, results='hide'}
MSFT <- data %>% 
  filter(COMPANY == 'MSFT') %>% 
  dplyr::select(EARNINGS_Scaled) %>% 
  arrange(EARNINGS_Scaled)

vector <- MSFT$EARNINGS_Scaled

myts <- ts(vector, start=c(1999), end=c(2021), frequency=1)

sf1_Microsofct = stan_sarima(ts = myts,iter = 2 * 5000)
```

```{r}
check_residuals(sf1_Microsofct)
autoplot(forecast(object = sf1_Microsofct,h = 12))
```

Looking at data from Microsoft, the most correlated value with the present day value is 1 year behind. Both ACF and PACF are consistent with this. Since we only have around 20, observations, we will not have more fluctuation so our EV of PPE. 


**American Airlines Prediction**

```{r, results='hide'}
AAL <- data %>% 
  filter(COMPANY == 'AAL') %>% 
  dplyr::select(EARNINGS_Scaled) %>% 
  arrange(EARNINGS_Scaled)

vector <- AAL$EARNINGS_Scaled

myts <- ts(vector, start=c(1999), end=c(2021), frequency=1)

sf1_aal = stan_sarima(ts = myts,iter = 2 * 5000)
```


```{r}
check_residuals(sf1_aal)
autoplot(forecast(object = sf1_aal,h = 12))
```

For American Airlines, we see a different story. As we can see from the graph, as airlines industry is an extremely cyclical field, the earnings fluctuate a lot. We could see that they fluctuate a lot during the previous years. With that in mind, the model predicts that American Airlines will not improve the much during the following years from 2021 to 2030 as predicting a cyclical company's earnings could be a really difficult story. It did not show the similar pattern as Amazon. 

Looking at the ACF lab plot, the most noticeable increase is when we go from 1 difference (so looking at two year behind) to looking at 2 difference. This suggests that particularly for American Airlines, the difference parameter should be 2. 


